# -*- coding: utf-8 -*-
"""suc_ragPDF_gguf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sb0T9ZVJ5VpC7vmcDgCI5wUn5LCRkI_U
"""







!wget https://huggingface.co/bartowski/Mistral-7B-Instruct-v0.3-GGUF/resolve/main/Mistral-7B-Instruct-v0.3-IQ1_M.gguf



!pip install numpy faiss-cpu torch pymupdf sentence-transformers llama-cpp-python python-docx

!pip install numpy faiss-cpu pymupdf sentence-transformers llama-cpp-python python-docx

import numpy as np
from faiss import IndexFlatL2
import torch
import os
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# تهيئة الجهاز
device = "cuda" if torch.cuda.is_available() else "cpu"

# تهيئة النماذج
MODEL_PATH = "mistral-7b-instruct-v0.1.Q4_K_M.gguf"  # مسار نموذج GGUF
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # نموذج التضمين المحلي

# تحميل نموذج LLM
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=40 if device == "cuda" else 0
)

# تحميل نموذج التضمين
embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)

# وظيفة لاستخراج النص من ملف PDF
def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# وظيفة توليد التضمينات النصية
def get_embeddings(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()

# وظيفة البحث عن المستند الأكثر صلة
def find_relevant_document(question, index, documents, file_names):
    question_embedding = get_embeddings([question])
    D, I = index.search(question_embedding, k=len(documents))
    return documents[I[0][0]], D[0][0], file_names[I[0][0]]

# وظيفة توليد الإجابة
def generate_answer(question, context):
    prompt = f"""السياق: {context}

السؤال: {question}
أجب على السؤال بناءً على السياق أعلاه. كن دقيقًا وواضحًا:"""

    response = llm.create_chat_completion(
        messages=[{"role": "user", "content": prompt}],
        max_tokens=512,
        temperature=0.2,
        top_p=0.9
    )

    return response['choices'][0]['message']['content']

# مثال على الاستخدام
if __name__ == "__main__":
    # 1. تحميل المستندات
    documents = []
    file_names = []
    file_paths = ["doc1.pdf", "doc2.docx"]  # استبدل بمسارات ملفاتك

    for file_path in file_paths:
        if file_path.endswith(".pdf"):
            text = extract_text_from_pdf(file_path)
        elif file_path.endswith(".docx"):
            import docx
            doc = docx.Document(file_path)
            text = "\n".join([para.text for para in doc.paragraphs])
        else:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()

        documents.append(text)
        file_names.append(os.path.basename(file_path))

    # 2. توليد التضمينات
    embeddings, index, _ = generate_embeddings(documents, file_names)

    # 3. طرح سؤال
    question = "ما هو الموضوع الرئيسي للمستند؟"

    # 4. البحث عن الإجابة
    relevant_doc, distance, file_name = find_relevant_document(question, index, documents, file_names)
    answer = generate_answer(question, relevant_doc)

    print(f"السؤال: {question}")
    print(f"الإجابة: {answer}")
    print(f"المستند المصدر: {file_name}")
    print(f"مسافة التشابه: {distance:.2f}")

import numpy as np
from faiss import IndexFlatL2
import torch
import os
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# تهيئة الجهاز
device = "cuda" if torch.cuda.is_available() else "cpu"

# تهيئة النماذج
MODEL_PATH = "/content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf"  # مسار نموذج GGUF
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # نموذج التضمين المحلي

# تحميل نموذج LLM
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=40 if device == "cuda" else 0
)

# تحميل نموذج التضمين
embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)

# وظيفة لاستخراج النص من ملف PDF
def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# وظيفة توليد التضمينات النصية
def get_embeddings(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()

# وظيفة البحث عن المستند الأكثر صلة
def find_relevant_document(question, index, documents, file_names):
    question_embedding = get_embeddings([question])
    D, I = index.search(question_embedding, k=len(documents))
    return documents[I[0][0]], D[0][0], file_names[I[0][0]]

# وظيفة توليد الإجابة
def generate_answer(question, context):
 prompt = f"""Context: {context}

Question: {question}
Answer the question based on the context above. Be precise and clear: """

    response = llm.create_chat_completion(
        messages=[{"role": "user", "content": prompt}],
        max_tokens=512,
        temperature=0.2,
        top_p=0.9
    )

    return response['choices'][0]['message']['content']

# مثال على الاستخدام
if __name__ == "__main__":
    # 1. تحميل المستندات
    documents = []
    file_names = []
    file_paths = ["/content/Understanding_Climate_Change.pdf", "/content/Product_Manual_Sample.docx"]  # استبدل بمسارات ملفاتك

    for file_path in file_paths:
        if file_path.endswith(".pdf"):
            text = extract_text_from_pdf(file_path)
        elif file_path.endswith(".docx"):
            import docx
            doc = docx.Document(file_path)
            text = "\n".join([para.text for para in doc.paragraphs])
        else:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()

        documents.append(text)
        file_names.append(os.path.basename(file_path))

    # 2. توليد التضمينات
    embeddings, index, _ = generate_embeddings(documents, file_names)

    # 3. طرح سؤال
    question = "What is the main topic of the document?"

    # 4. البحث عن الإجابة
    relevant_doc, distance, file_name = find_relevant_document(question, index, documents, file_names)
    answer = generate_answer(question, relevant_doc)

    print(f"Question: {question}")
    print(f"Answer: {answer}")
    print(f"Source Document: {file_name}")
    print(f"Similarity Distance: {distance:.2f}")

import numpy as np
from faiss import IndexFlatL2
import torch
import os
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# تهيئة الجهاز
device = "cuda" if torch.cuda.is_available() else "cpu"

# تهيئة النماذج
MODEL_PATH = "/content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf"  # مسار نموذج GGUF
EMBEDDING_MODEL = "all-MiniLM-L6-v2"  # نموذج التضمين المحلي

# تحميل نموذج LLM
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=40 if device == "cuda" else 0
)

# تحميل نموذج التضمين
embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)

# وظيفة لاستخراج النص من ملف PDF
def extract_text_from_pdf(file_path):
    doc = fitz.open(file_path)
    text = ""
    for page in doc:
        text += page.get_text()
    return text

# وظيفة توليد التضمينات النصية
def get_embeddings(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()

# وظيفة توليد التضمينات وحفظها في Faiss
def generate_embeddings(documents, file_names):
    embeddings = get_embeddings(documents)
    index = IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return embeddings, index, file_names

# وظيفة البحث عن المستند الأكثر صلة
def find_relevant_document(question, index, documents, file_names):
    question_embedding = get_embeddings([question])
    D, I = index.search(question_embedding, k=len(documents))
    return documents[I[0][0]], D[0][0], file_names[I[0][0]]

# وظيفة توليد الإجابة
def generate_answer(question, context):
    prompt = f"""Context: {context}

Question: {question}
Answer the question based on the context above. Be precise and clear:"""

    response = llm.create_chat_completion(
        messages=[{"role": "user", "content": prompt}],
        max_tokens=512,
        temperature=0.2,
        top_p=0.9
    )

    return response['choices'][0]['message']['content']

# مثال على الاستخدام
if __name__ == "__main__":
    # 1. تحميل المستندات
    documents = []
    file_names = []
    file_paths = ["/content/Understanding_Climate_Change.pdf", "/content/Product_Manual_Sample.docx"]  # استبدل بمسارات ملفاتك

    for file_path in file_paths:
        try:
            if file_path.endswith(".pdf"):
                text = extract_text_from_pdf(file_path)
            elif file_path.endswith(".docx"):
                import docx
                doc = docx.Document(file_path)
                text = "\n".join([para.text for para in doc.paragraphs])
            else:
                with open(file_path, "r", encoding="utf-8") as f:
                    text = f.read()

            if text.strip():  # تأكد من أن النص ليس فارغًا
                documents.append(text)
                file_names.append(os.path.basename(file_path))
            else:
                print(f"Warning: Empty document - {file_path}")
        except Exception as e:
            print(f"Error processing {file_path}: {str(e)}")

    if not documents:
        print("No valid documents found. Exiting.")
        exit()

    # 2. توليد التضمينات
    embeddings, index, _ = generate_embeddings(documents, file_names)

    # 3. طرح سؤال
    question = "What is the main topic of the document?"

    # 4. البحث عن الإجابة
    relevant_doc, distance, file_name = find_relevant_document(question, index, documents, file_names)
    answer = generate_answer(question, relevant_doc)

    print("\n" + "="*50)
    print(f"Question: {question}")
    print("="*50)
    print(f"Answer:\n{answer}")
    print("-"*50)
    print(f"Source Document: {file_name}")
    print(f"Similarity Distance: {distance:.2f}")
    print("="*50 + "\n")

"""aشغال جيد"""

import numpy as np
from faiss import IndexFlatL2
import torch
import os
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# تهيئة الجهاز
device = "cuda" if torch.cuda.is_available() else "cpu"

# تهيئة النماذج
MODEL_PATH = "/content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# تحميل النماذج
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=40 if device == "cuda" else 0
)

embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)

# وظيفة لاستخراج النص مع تقسيمه إلى أجزاء
def extract_text(file_path, max_length=10000):
    text = ""
    try:
        if file_path.endswith(".pdf"):
            doc = fitz.open(file_path)
            for page in doc:
                text += page.get_text() + "\n"
                if len(text) > max_length:
                    break
        elif file_path.endswith(".docx"):
            import docx
            doc = docx.Document(file_path)
            for para in doc.paragraphs:
                text += para.text + "\n"
                if len(text) > max_length:
                    break
        else:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read(max_length)
    except Exception as e:
        print(f"Error processing {file_path}: {str(e)}")
    return text[:max_length]

# وظائف التضمين والبحث
def get_embeddings(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()

def generate_embeddings(documents, file_names):
    embeddings = get_embeddings(documents)
    index = IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return embeddings, index, file_names

def find_relevant_document(question, index, documents, file_names):
    question_embedding = get_embeddings([question])
    D, I = index.search(question_embedding, k=len(documents))
    return documents[I[0][0]], D[0][0], file_names[I[0][0]]

# وظيفة توليد الإجابة مع التحكم في الطول
def generate_answer(question, context, max_context_length=1500):
    # تقليل حجم السياق إذا كان طويلاً جداً
    if len(context) > max_context_length:
        context = context[:max_context_length] + "... [text truncated]"

    prompt = f"""Context: {context}\n\nQuestion: {question}\nAnswer:"""

    try:
        response = llm.create_chat_completion(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=512,
            temperature=0.2,
            top_p=0.9
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"Error generating answer: {str(e)}"

# التنفيذ الرئيسي
if __name__ == "__main__":
    file_paths = ["/content/Understanding_Climate_Change.pdf", "/content/Product_Manual_Sample.docx"]

    # معالجة الملفات
    documents = []
    file_names = []
    for file_path in file_paths:
        text = extract_text(file_path)
        if text.strip():
            documents.append(text)
            file_names.append(os.path.basename(file_path))

    if not documents:
        print("No valid documents found.")
        exit()

    # توليد التضمينات
    embeddings, index, _ = generate_embeddings(documents, file_names)

    # طرح الأسئلة
    questions = [
        "What is the main topic of the document?",
        "Summarize the key points in 3 bullet points",
        "What are the most important conclusions?"
    ]

    for question in questions:
        try:
            relevant_doc, distance, file_name = find_relevant_document(question, index, documents, file_names)
            answer = generate_answer(question, relevant_doc)

            print("\n" + "="*60)
            print(f"Question: {question}")
            print("-"*60)
            print(f"Answer:\n{answer}")
            print("-"*60)
            print(f"Source: {file_name} | Similarity: {distance:.2f}")
            print("="*60 + "\n")
        except Exception as e:
            print(f"Error processing question '{question}': {str(e)}")



"""https://chat.deepseek.com/a/chat/s/00311bb7-fb09-41d7-9626-b01e4573af34

التحسينات الرئيسية:

تقسيم النص إلى أجزاء:

أضفت معلمة max_length لتحديد الحد الأقصى للنص المستخرج

إيقاف استخراج النص عند الوصول إلى الحد الأقصى

تحسين معالجة الأخطاء:

معالجة أفضل للأخطاء أثناء استخراج النص

رسائل خطأ واضحة

التحكم في طول السياق:

تقليم النص الطويل قبل إرساله للنموذج

إضافة تنبيه عندما يتم تقليم النص

تحسين الإخراج:

تنسيق أوضح للإجابات

عرض درجة التشابه مع المستند

إضافة أمثلة أسئلة:

قائمة أسئلة افتراضية لتجربة النظام

نصائح للاستخدام:

يمكنك ضبط max_length في دالة extract_text للتحكم في كمية النص المستخرجة

يمكنك تعديل max_context_length في دالة generate_answer للتحكم في كمية النص المرسلة للنموذج

أضف المزيد من معالجة الأخطاء حسب احتياجاتك

هذا الكود الآن أكثر قوة في التعامل مع المستندات الكبيرة ويوفر حماية أفضل ضد تجاوز حدود السياق المسموح به.
"""





"""شغال جيد"""

import numpy as np
from faiss import IndexFlatL2
import torch
import os
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from llama_cpp import Llama

# تهيئة الجهاز
device = "cuda" if torch.cuda.is_available() else "cpu"

# تهيئة النماذج
MODEL_PATH = "/content/Mistral-7B-Instruct-v0.3-IQ1_M.gguf"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"

# تحميل النماذج
llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=4,
    n_gpu_layers=40 if device == "cuda" else 0
)

embedding_model = SentenceTransformer(EMBEDDING_MODEL, device=device)

# وظيفة لاستخراج النص مع تقسيمه إلى أجزاء
def extract_text(file_path, max_length=10000):
    text = ""
    try:
        if file_path.endswith(".pdf"):
            doc = fitz.open(file_path)
            for page in doc:
                text += page.get_text() + "\n"
                if len(text) > max_length:
                    break
        elif file_path.endswith(".docx"):
            import docx
            doc = docx.Document(file_path)
            for para in doc.paragraphs:
                text += para.text + "\n"
                if len(text) > max_length:
                    break
        else:
            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read(max_length)
    except Exception as e:
        print(f"Error processing {file_path}: {str(e)}")
    return text[:max_length]

# وظائف التضمين والبحث
def get_embeddings(texts):
    return embedding_model.encode(texts, convert_to_tensor=True).cpu().numpy()

def generate_embeddings(documents, file_names):
    embeddings = get_embeddings(documents)
    index = IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    return embeddings, index, file_names

def find_relevant_document(question, index, documents, file_names):
    question_embedding = get_embeddings([question])
    D, I = index.search(question_embedding, k=len(documents))
    return documents[I[0][0]], D[0][0], file_names[I[0][0]]

# وظيفة توليد الإجابة مع التحكم في الطول
def generate_answer(question, context, max_context_length=1500):
    # تقليل حجم السياق إذا كان طويلاً جداً
    if len(context) > max_context_length:
        context = context[:max_context_length] + "... [text truncated]"

    prompt = f"""Context: {context}\n\nQuestion: {question}\nAnswer:"""

    try:
        response = llm.create_chat_completion(
            messages=[{"role": "user", "content": prompt}],
            max_tokens=512,
            temperature=0.2,
            top_p=0.9
        )
        return response['choices'][0]['message']['content']
    except Exception as e:
        return f"Error generating answer: {str(e)}"

# التنفيذ الرئيسي
if __name__ == "__main__":
    file_paths = ["/content/Understanding_Climate_Change.pdf", "/content/Product_Manual_Sample.docx"]

    # معالجة الملفات
    documents = []
    file_names = []
    for file_path in file_paths:
        text = extract_text(file_path)
        if text.strip():
            documents.append(text)
            file_names.append(os.path.basename(file_path))

    if not documents:
        print("No valid documents found.")
        exit()

    # توليد التضمينات
    embeddings, index, _ = generate_embeddings(documents, file_names)

    # طرح الأسئلة
    questions = [
        "What is the main topic of the document?",
        "Summarize the key points in 3 bullet points",
        "What are the most important conclusions?"
    ]

    for question in questions:
        try:
            relevant_doc, distance, file_name = find_relevant_document(question, index, documents, file_names)
            answer = generate_answer(question, relevant_doc)

            print("\n" + "="*60)
            print(f"Question: {question}")
            print("-"*60)
            print(f"Answer:\n{answer}")
            print("-"*60)
            print(f"Source: {file_name} | Similarity: {distance:.2f}")
            print("="*60 + "\n")
        except Exception as e:
            print(f"Error processing question '{question}': {str(e)}")