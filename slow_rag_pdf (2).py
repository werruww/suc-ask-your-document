# -*- coding: utf-8 -*-
"""slow_rag_pdf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dkL841zjjUwmC4PSR0H2NGplZjmhAN8s
"""







# Commented out IPython magic to ensure Python compatibility.
!git clone https://huggingface.co/spaces/manasvipatwa/RAG
# %cd RAG

!pip install -r requirements.txt

!pip install pyngrok

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &

!nohup ollama serve &

!ollama pull llama3.2

!ollama list

!huggingface-cli login

!pip install pyngrok

2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d


from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(8501).public_url
    print(f"üöÄ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!python app.py

from pyngrok import ngrok
import threading
import time

ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(8501).public_url
    print(f"üöÄ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!streamlit run /content/RAG/app.py

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!wget https://huggingface.co/bartowski/Dolphin3.0-Llama3.2-1B-GGUF/resolve/main/Dolphin3.0-Llama3.2-1B-IQ2_M.gguf

!nohup ollama serve &

!ollama list

!ollama create a -f a

!ollama list

from pyngrok import ngrok
import threading
import time
!huggingface-cli login --token XXXXX
ngrok_token = "2qoMyvN33sChLDfKL39OR8IDIxk_2hoKPdPQp4MUdSQbDPv6d"
ngrok.set_auth_token(ngrok_token)

# Start ngrok in a separate thread to avoid blocking
def start_ngrok():
    public_url = ngrok.connect(8501).public_url
    print(f"üöÄ Ngrok Tunnel Open: {public_url}")

ngrok_thread = threading.Thread(target=start_ngrok)
ngrok_thread.start()

# Wait for ngrok to start (optional)
time.sleep(5)

# Execute your node.js script
!streamlit run /content/RAG/app.py

!ngrok http 8501

!nohup ollama serve &

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!git clone https://huggingface.co/cognitivecomputations/Dolphin3.0-Llama3.2-1B

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!git clone https://huggingface.co/tiiuae/falcon-rw-1b
from transformers import AutoTokenizer, AutoModelForCausalLM
model_hf_path = "tiiuae/falcon-rw-1b"
tokenizer = AutoTokenizer.from_pretrained(model_hf_path)
model = AutoModelForCausalLM.from_pretrained(model_hf_path)

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!git clone https://huggingface.co/tiiuae/falcon-rw-1b

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

model_path = "/content/falcon-rw-1b"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)
model.to(device)

prompt = "who is ai?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

model_path = "/content/Dolphin3.0-Llama3.2-1B"

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path)
model.to(device)

prompt = "who is python?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
outputs = model.generate(**inputs, max_new_tokens=11)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)







# Commented out IPython magic to ensure Python compatibility.
# %cd /content/RAG

#!npm install -g localtunnel
!streamlit run app.py &
!lt --port 8501

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/RAG

#!npm install -g localtunnel
!streamlit run app.py && lt --port 8501
!lt --port 8501

!npx localtunnel --port 8501

# Commented out IPython magic to ensure Python compatibility.

# %cd /content/RAG
!streamlit run app.py
!npx localtunnel --port 8501

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/RAG
!streamlit run app.py && npx localtunnel --port 8501
!npx localtunnel --port 8501

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/RAG
!streamlit run app.py &

!npx localtunnel --port 8501

"""https://discuss.streamlit.io/t/how-to-launch-streamlit-app-from-google-colab-notebook/42399"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import streamlit as st
# 
# st.write('Hello, *World!* :sunglasses:')

!streamlit run app.py &>/content/logs.txt &

!npx localtunnel --port 8501 & curl ipv4.icanhazip.com

#!pip install pyngrok
from pyngrok import ngrok
public_url = ngrok.connect(8501)
print(public_url)
!streamlit run app.py &

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/RAG

!streamlit run app.py &>/content/logs.txt &

!npx localtunnel --port 8501 & curl ipv4.icanhazip.com













# Commented out IPython magic to ensure Python compatibility.
# %cd /content/RAG

!streamlit run /content/RAG/app.py --server.address 0.0.0.0 --server.port 8501 > /dev/null 2>&1 &

from pyngrok import ngrok

# ÿßŸÅÿ™ÿ≠ ŸÜŸÅŸÇ ŸÑŸÑŸÖŸÜŸÅÿ∞ 8501
public_url = ngrok.connect(8501)
print("üîó ÿ±ÿßÿ®ÿ∑ ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ:", public_url)

pip install numpy faiss-cpu torch transformers sentence-transformers PyMuPDF pandas python-docx python-pptx firecrawl-py opik-python smolagents openpyxl # Added openpyxl for excel
# Or faiss-gpu if you have CUDA setup correctly
# pip install faiss-gpu torch transformers sentence-transformers PyMuPDF pandas python-docx python-pptx firecrawl-py opik-python smolagents openpyxl

"""ÿ¥ÿ∫ÿßŸÑ ÿ±ÿßÿ¨ ŸÉÿ™ÿßÿ®"""

import numpy as np
from faiss import IndexFlatL2
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
import torch
import os
import requests
import opik
from opik import track # Assuming opik_context is not strictly needed without UI interaction tracking
import fitz # PyMuPDF
import pandas as pd
from io import StringIO, BytesIO # Added BytesIO for binary reads if needed
import docx
from pptx import Presentation
import json
from firecrawl import FirecrawlApp
from IPython.display import Markdown, display # Note: display won't work in a standard script, use print
import time
import base64
!huggingface-cli login --token XXXXX
# Removed phi imports as the agent used is CodeAgent with HfApiModel
# from phi.agent import Agent, RunResponse
# from phi.model.huggingface import HuggingFaceChat
# from phi.utils.pprint import pprint_run_response
# from phi.model.openai import OpenAIChat
# Removed validator/guardrails/phoenix imports as they weren't used in the provided core logic
# from validator import LlmRagEvaluator, HallucinationPrompt, QACorrectnessPrompt
# from guardrails import Guard
# import phoenix.evals
from smolagents import CodeAgent, HfApiModel
# Removed streamlit specific imports
# from streamlit_pdf_viewer import pdf_viewer
# from streamlit import session_state as ss
# from phoenix.evals import (
#     QA_PROMPT_RAILS_MAP,
#     QA_PROMPT_TEMPLATE,
#     OpenAIModel,
#     llm_classify,
# )

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Configuration ---
# Set these environment variables before running, or replace os.getenv with actual paths/keys
MODEL_PATH = os.getenv("MODEL_PATH", "sentence-transformers/all-MiniLM-L6-v2") # Example default
HF_MODEL_PATH = os.getenv("HF_MODEL_PATH", "google/gemma-2b-it") # Example default for QA model
FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API")
OPIK_API_KEY = os.getenv("OPIK_API_KEY") # Needed if Opik tracking is enabled

# --- Initialize Models ---
print("Loading models...")
try:
    # For embeddings
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" # Keep this if needed
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModel.from_pretrained(MODEL_PATH)
    model.to(device) # Move embedding model to device

    # For QA generation (using smolagents HfApiModel implicitly uses Transformers pipeline)
    # We still need the model path for HfApiModel if it doesn't default correctly
    # Note: HfApiModel might handle model loading internally based on its config.
    # The explicit loading of model_hf/tokenizer_hf might be redundant depending on HfApiModel's implementation.
    # If HfApiModel requires explicit model/tokenizer, pass them during its initialization.
    # tokenizer_hf = AutoTokenizer.from_pretrained(HF_MODEL_PATH)
    # model_hf = AutoModelForCausalLM.from_pretrained(HF_MODEL_PATH)
    # model_hf.to(device) # Move QA model to device if loaded explicitly

    # Initialize Opik if API key is available
    if OPIK_API_KEY:
        opik.init(api_key=OPIK_API_KEY)
    else:
        print("Opik API key not found. Tracking disabled.")
        # Define a dummy decorator if Opik is not initialized
        def track(*args, **kwargs):
            def decorator(func):
                return func
            return decorator

except Exception as e:
    print(f"Error loading models: {e}")
    print("Please ensure MODEL_PATH and HF_MODEL_PATH environment variables are set correctly.")
    exit()
print("Models loaded.")

# --- Core Functions (Unchanged) ---

def get_embeddings(texts):
    # Ensure model is on the correct device
    model.to(device)
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(device)
    with torch.no_grad():
        # model output needs to be checked, might require specific pooling
        outputs = model(**inputs)
        # Assuming mean pooling of the last hidden state
        embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.cpu().numpy()

def generate_embeddings(documents, file_names):
    print(f"Generating embeddings for {len(documents)} documents...")
    embeddings = get_embeddings(documents)
    index = IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    print("Embeddings generated and indexed.")
    return embeddings, index, file_names

def find_relevant_document(question, faiss_index, documents, file_names):
    print(f"Searching for relevant document for question: '{question}'")
    question_embedding = get_embeddings([question])
    # Ensure k is not greater than the number of documents in the index
    k = min(len(documents), faiss_index.ntotal)
    if k == 0:
        print("Warning: No documents in the index to search.")
        return None, float('inf'), None
    D, I = faiss_index.search(question_embedding, k=k)
    if I.size == 0 or I[0][0] < 0:
         print("Warning: Faiss search returned invalid index.")
         return None, float('inf'), None
    best_doc_index = I[0][0]
    file_name = file_names[best_doc_index]
    distance = D[0][0]
    print(f"Found relevant document: '{file_name}' (Index: {best_doc_index}, Distance: {distance:.4f})")
    return documents[best_doc_index], distance, file_name

@track(project_name="Document QA Agent Script", tags=['agent', 'python-library', 'querying'])
def generate_answer_with_agent(agent, question, relevant_doc):
    prompt = f"Based *only* on the following document, answer the question.\n\nDocument:\n{relevant_doc}\n\nQuestion: {question}\n\nAnswer:"
    print("Generating answer...")
    try:
        # Assuming agent.run returns a string or an object with a string representation
        response = agent.run(prompt)
        # Adjust based on the actual return type of agent.run
        if isinstance(response, str):
            answer = response
        elif hasattr(response, 'content'): # Example if it returns an object
             answer = response.content
        elif hasattr(response, 'text'): # Another common attribute
             answer = response.text
        else:
             answer = str(response) # Fallback
        print("Answer generated.")
        return answer
    except Exception as e:
        print(f"Error during answer generation: {e}")
        return f"Error generating answer: {e}"


# --- File Processing Helper Functions ---

def extract_text_from_pdf(file_path):
    try:
        with fitz.open(file_path) as pdf_file:
            text = ""
            for page in pdf_file:
                text += page.get_text()
            return text
    except Exception as e:
        print(f"Error reading PDF {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_docx(file_path):
    try:
        doc = docx.Document(file_path)
        text = ""
        for para in doc.paragraphs:
            text += para.text + "\n"
        return text
    except Exception as e:
        print(f"Error reading DOCX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_excel(file_path):
    try:
        df = pd.read_excel(file_path, sheet_name=None) # Read all sheets
        text = ""
        for sheet_name, sheet_df in df.items():
            text += f"--- Sheet: {sheet_name} ---\n"
            # Improved: Convert dataframe to string representation
            text += sheet_df.to_string(index=False) + "\n\n"
            # Old way (might lose structure):
            # for col in sheet_df.columns:
            #     # Handle potential non-string data robustly
            #     text += "\n".join(sheet_df[col].astype(str).fillna('')) + "\n"
        return text
    except Exception as e:
        print(f"Error reading Excel {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_pptx(file_path):
    try:
        prs = Presentation(file_path)
        text = ""
        for i, slide in enumerate(prs.slides):
            text += f"--- Slide {i+1} ---\n"
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    text += shape.text + "\n"
            # Check for notes
            if slide.has_notes_slide:
                 notes_slide = slide.notes_slide
                 notes_text = notes_slide.notes_text_frame.text
                 if notes_text:
                     text += f"\nNotes:\n{notes_text}\n"
            text += "\n" # Separator between slides
        return text
    except Exception as e:
        print(f"Error reading PPTX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_txt(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        print(f"Error reading TXT {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_csv(file_path):
    try:
        df = pd.read_csv(file_path)
        # Convert entire DataFrame to string for context
        return df.to_string(index=False)
    except Exception as e:
        print(f"Error reading CSV {os.path.basename(file_path)}: {e}")
        return None

# --- Main Processing Logic ---

def process_documents(file_paths, questions_to_ask):
    """
    Processes a list of local document files, generates embeddings,
    and answers questions based on the most relevant document.
    """
    all_documents = []
    file_names = []

    print("\n--- Processing Local Documents ---")
    print(f"Files to process: {file_paths}")

    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"Warning: File not found - {file_path}. Skipping.")
            continue

        file_name = os.path.basename(file_path)
        print(f"Reading file: {file_name}")
        _, ext = os.path.splitext(file_path)
        doc_text = None

        if ext == ".pdf":
            doc_text = extract_text_from_pdf(file_path)
        elif ext == ".docx":
            doc_text = extract_text_from_docx(file_path)
        elif ext == ".txt":
            doc_text = extract_text_from_txt(file_path)
        elif ext in [".xlsx", ".xls"]:
            doc_text = extract_text_from_excel(file_path)
        elif ext == ".pptx":
            doc_text = extract_text_from_pptx(file_path)
        elif ext == ".csv":
            doc_text = extract_text_from_csv(file_path)
        else:
            print(f"Warning: Unsupported file type '{ext}' for {file_name}. Skipping.")
            continue

        if doc_text:
            all_documents.append(doc_text)
            file_names.append(file_name)
            print(f"Successfully processed and added text from {file_name}.")
        else:
             print(f"Failed to extract text from {file_name}.")


    if not all_documents:
        print("No documents were successfully processed. Exiting document processing.")
        return

    # Generate embeddings
    embeddings, index, processed_file_names = generate_embeddings(all_documents, file_names)

    # Initialize QA Agent
    # Ensure HfApiModel is configured correctly (e.g., with HF_MODEL_PATH if needed)
    # agent = CodeAgent(tools=[], model=HfApiModel(model=HF_MODEL_PATH)) # Pass model path if needed
    agent = CodeAgent(tools=[], model=HfApiModel()) # Assuming default works or it reads from env


    # Ask questions
    print("\n--- Answering Questions for Documents ---")
    results = []
    for question in questions_to_ask:
        relevant_doc, distance, rel_file_name = find_relevant_document(question, index, all_documents, processed_file_names)

        if relevant_doc is None:
            print(f"Could not find relevant document for question: '{question}'")
            answer = "Could not find a relevant document to answer the question."
            rel_file_name = "N/A"
            distance = "N/A"
        else:
            answer = generate_answer_with_agent(agent, question, relevant_doc)

        print("-" * 20)
        print(f"Question: {question}")
        print(f"Most Relevant File: {rel_file_name}")
        # print(f"Relevance Score (Distance): {distance if isinstance(distance, str) else f'{distance:.4f}'}") # Check type before formatting
        print(f"Answer:\n{answer}")
        print("-" * 20)
        results.append({
            'question': question,
            'file': rel_file_name,
            'distance': distance,
            'answer': answer
        })
    return results


def process_website(url, questions_to_ask):
    """
    Processes a website URL using Firecrawl, generates embeddings for the
    scraped content, and answers questions.
    """
    print("\n--- Processing Website ---")
    print(f"URL to process: {url}")

    if not FIRECRAWL_API_KEY:
        print("Error: Firecrawl API key not set (FIRECRAWL_API environment variable). Skipping website processing.")
        return None

    if not url:
        print("Error: No URL provided for website processing.")
        return None

    documents = []
    file_names = [url] # Use the URL as the identifier

    try:
        print("Initializing Firecrawl...")
        app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)
        print(f"Scraping URL: {url}...")
        # Request markdown format for text extraction
        scrape_result = app.scrape_url(url, params={'pageOptions': {'onlyMainContent': True}}) # Focus on main content

        # Firecrawl V0.2.2 returns a dictionary with a 'markdown' or 'content' key
        scraped_text = None
        if isinstance(scrape_result, dict):
            if 'markdown' in scrape_result and scrape_result['markdown']:
                 scraped_text = scrape_result['markdown']
                 print("Extracted Markdown content.")
            elif 'content' in scrape_result and scrape_result['content']: # Fallback for older versions or different configs
                 scraped_text = scrape_result['content']
                 print("Extracted 'content' field.")
            else:
                 print("Warning: Could not find 'markdown' or 'content' in scrape result.")
                 print("Full scrape result:", scrape_result) # Log for debugging

        else:
            # Handle unexpected result format
             print(f"Warning: Unexpected scrape result format: {type(scrape_result)}")
             print("Full scrape result:", scrape_result)


        if scraped_text:
            documents.append(scraped_text)
            print(f"Successfully scraped content from {url}.")
        else:
            print(f"No valid content extracted for {url}. Please check the URL and Firecrawl result.")
            return None # Can't proceed without content

    except Exception as e:
        print(f"Error during website scraping for {url}: {e}")
        return None

    if not documents:
        print("No content scraped from the website. Cannot proceed.")
        return None

    # Generate embeddings for the single scraped document
    embeddings, index, processed_file_names = generate_embeddings(documents, file_names)

    # Initialize QA Agent
    agent = CodeAgent(tools=[], model=HfApiModel()) # Assuming default works

    # Ask questions
    print("\n--- Answering Questions for Website ---")
    results = []
    for question in questions_to_ask:
        # Since there's only one document, Faiss search is trivial but keeps the flow consistent
        relevant_doc, distance, rel_file_name = find_relevant_document(question, index, documents, processed_file_names)

        if relevant_doc is None:
             # This shouldn't happen if scraping succeeded and indexing worked, but handle defensively
             print(f"Error retrieving the scraped document for question: '{question}'")
             answer = "Could not retrieve the scraped document."
             rel_file_name = url # Best identifier we have
             distance = "N/A"
        else:
            answer = generate_answer_with_agent(agent, question, relevant_doc)

        print("-" * 20)
        print(f"Question: {question}")
        print(f"Source URL: {rel_file_name}")
        # print(f"Relevance Score (Distance): {distance if isinstance(distance, str) else f'{distance:.4f}'}") # Check type
        print(f"Answer:\n{answer}")
        print("-" * 20)

        results.append({
            'question': question,
            'url': rel_file_name,
            'answer': answer
        })
    return results

# --- Main Execution ---
if __name__ == "__main__":
    # --- Define Inputs Here ---

    # 1. Document Processing
    # List of local file paths to process
    document_files = [
        # Add paths to your local files, e.g.:
        # "/path/to/your/document1.pdf",
        "/content/Product_Manual_Sample.docx",
        # "data/presentation.pptx",
        # "financials.xlsx"
    ]
    # Questions to ask about the documents
    document_questions = [
        "What is the main topic discussed in the documents?",
        "Summarize the key findings.",
        # Add more specific questions relevant to your documents
        # e.g., "What were the Q3 financial results according to financials.xlsx?"
        # Note: The current `find_relevant_document` finds the *single* best match across *all* docs.
        # For questions targeting a specific file, you might need to adapt the logic or rely on the LLM
        # understanding the file name context if included in the question (less reliable).
    ]

    # 2. Website Processing
    # URL of the website to process
    website_url = "https://github.com/localtunnel/localtunnel" # Replace with the target URL
    # Questions to ask about the website
    website_questions = [
        "What is the main purpose of this website?",
        "Summarize the content of the homepage.",
        "What services or products are offered?",
    ]

    # --- Run Processing ---

    if document_files:
        document_results = process_documents(document_files, document_questions)
        if document_results:
            print("\n=== Document Processing Summary ===")
            # Optionally save results to a file (e.g., JSON)
            # with open("document_qa_results.json", "w") as f:
            #     json.dump(document_results, f, indent=4)
            print("Document processing finished.")
        else:
            print("\nDocument processing did not yield results.")

    if website_url:
        website_results = process_website(website_url, website_questions)
        if website_results:
            print("\n=== Website Processing Summary ===")
            # Optionally save results
            # with open("website_qa_results.json", "w") as f:
            #     json.dump(website_results, f, indent=4)
            print("Website processing finished.")
        else:
             print("\nWebsite processing did not yield results.")

    print("\nScript finished.")

import numpy as np
from faiss import IndexFlatL2
from transformers import AutoTokenizer, AutoModel # Removed AutoModelForCausalLM as it's handled by HfApiModel
import torch
import os
import requests
# Removed opik imports as tracking might not be needed/setup in this context
# import opik
# from opik import track
import fitz # PyMuPDF
import pandas as pd
from io import StringIO, BytesIO
import docx
from pptx import Presentation
import json
from firecrawl import FirecrawlApp
# Removed IPython display as it's for notebooks
# from IPython.display import Markdown, display
import time
import base64
from smolagents import CodeAgent, HfApiModel # Keep smolagents
# Removed unused phi/validator/guardrails/phoenix imports
# Removed streamlit specific imports

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# --- Configuration ---
# Set these environment variables before running, or replace os.getenv with actual paths/keys
# Embedding Model Path (Keep or change as needed)
MODEL_PATH = os.getenv("MODEL_PATH", "sentence-transformers/all-MiniLM-L6-v2")

# === MODIFIED PART ===
# QA Generation Model Path - Set to the specific path requested
HF_MODEL_PATH = "/content/Dolphin3.0-Llama3.2-1B"
print(f"Using QA Model Path: {HF_MODEL_PATH}")
# Ensure this path is accessible in your execution environment (e.g., model downloaded/mounted at /content/)
# ====================

FIRECRAWL_API_KEY = os.getenv("FIRECRAWL_API")
# OPIK_API_KEY = os.getenv("OPIK_API_KEY") # Removed Opik

# --- Initialize Embedding Model ---
print("Loading embedding model...")
try:
    # For embeddings
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" # Keep this if needed
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModel.from_pretrained(MODEL_PATH)
    model.to(device) # Move embedding model to device
    print("Embedding model loaded.")
except Exception as e:
    print(f"Error loading embedding model ({MODEL_PATH}): {e}")
    print("Please ensure the embedding MODEL_PATH is set correctly and the model exists.")
    exit()

# Note: QA Model (Dolphin) is loaded implicitly by HfApiModel below

# Define a dummy track decorator if Opik is not used
def track(*args, **kwargs):
    def decorator(func):
        return func
    return decorator

# --- Core Functions (Unchanged) ---

def get_embeddings(texts):
    # Ensure model is on the correct device
    model.to(device)
    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1)
    return embeddings.cpu().numpy()

def generate_embeddings(documents, file_names):
    print(f"Generating embeddings for {len(documents)} documents using {MODEL_PATH}...")
    embeddings = get_embeddings(documents)
    if embeddings.shape[0] == 0:
        print("Warning: No embeddings generated (perhaps no documents?). Cannot create index.")
        return None, None, []
    index = IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    print("Embeddings generated and indexed.")
    return embeddings, index, file_names

def find_relevant_document(question, faiss_index, documents, file_names):
    if faiss_index is None or faiss_index.ntotal == 0:
        print("Warning: Faiss index is not available or empty. Cannot search.")
        return None, float('inf'), None
    print(f"Searching for relevant document for question: '{question}'")
    question_embedding = get_embeddings([question])
    k = min(len(documents), faiss_index.ntotal) # Ensure k is valid
    if k == 0:
        print("Warning: No documents in the index to search.")
        return None, float('inf'), None
    try:
        D, I = faiss_index.search(question_embedding, k=k)
        if I.size == 0 or I[0][0] < 0 or I[0][0] >= len(documents):
             print(f"Warning: Faiss search returned invalid index: {I}. Index size: {faiss_index.ntotal}, Docs length: {len(documents)}")
             return None, float('inf'), None
        best_doc_index = I[0][0]
        file_name = file_names[best_doc_index]
        distance = D[0][0]
        print(f"Found relevant document: '{file_name}' (Index: {best_doc_index}, Distance: {distance:.4f})")
        return documents[best_doc_index], distance, file_name
    except Exception as e:
        print(f"Error during Faiss search: {e}")
        return None, float('inf'), None


@track(project_name="Document QA Agent Script", tags=['agent', 'python-library', 'querying']) # Dummy decorator
def generate_answer_with_agent(agent, question, relevant_doc):
    prompt = f"Based *only* on the following document, answer the question.\n\nDocument:\n{relevant_doc}\n\nQuestion: {question}\n\nAnswer:"
    print(f"Generating answer using agent with model: {agent.model.model if hasattr(agent.model, 'model') else 'N/A'}...") # Check agent model attr
    try:
        # Assuming agent.run returns a string or an object with a string representation
        response = agent.run(prompt)
        # Adjust based on the actual return type of agent.run
        if isinstance(response, str):
            answer = response
        elif hasattr(response, 'content'):
             answer = response.content
        elif hasattr(response, 'text'):
             answer = response.text
        else:
             answer = str(response) # Fallback
        print("Answer generated.")
        return answer
    except Exception as e:
        print(f"Error during answer generation: {e}")
        # Optionally include more details if available, e.g., from the exception object
        return f"Error generating answer: {e}"


# --- File Processing Helper Functions (Unchanged) ---

def extract_text_from_pdf(file_path):
    try:
        with fitz.open(file_path) as pdf_file:
            text = ""
            for page in pdf_file:
                text += page.get_text()
            return text
    except Exception as e:
        print(f"Error reading PDF {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_docx(file_path):
    try:
        doc = docx.Document(file_path)
        text = ""
        for para in doc.paragraphs:
            text += para.text + "\n"
        return text
    except Exception as e:
        print(f"Error reading DOCX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_excel(file_path):
    try:
        # Read all sheets into a dictionary of DataFrames
        excel_file = pd.ExcelFile(file_path)
        text = ""
        for sheet_name in excel_file.sheet_names:
            df = excel_file.parse(sheet_name)
            # Add sheet name as context
            text += f"--- Sheet: {sheet_name} ---\n"
            # Convert dataframe to a string format suitable for LLM context
            # Using to_string is often better than iterating columns
            text += df.to_string(index=False, na_rep='NA').strip() + "\n\n"
        return text.strip() if text else None
    except Exception as e:
        print(f"Error reading Excel {os.path.basename(file_path)}: {e}")
        return None


def extract_text_from_pptx(file_path):
    try:
        prs = Presentation(file_path)
        text = ""
        for i, slide in enumerate(prs.slides):
            text += f"--- Slide {i+1} ---\n"
            slide_texts = []
            for shape in slide.shapes:
                if hasattr(shape, "text"):
                    # Append shape text, handle potential None or empty strings
                     shape_text = shape.text.strip() if shape.text else ""
                     if shape_text:
                         slide_texts.append(shape_text)

            # Check for notes
            if slide.has_notes_slide:
                 notes_slide = slide.notes_slide
                 if notes_slide.notes_text_frame and notes_slide.notes_text_frame.text:
                    notes_text = notes_slide.notes_text_frame.text.strip()
                    if notes_text:
                         text += f"Slide Text:\n" + "\n".join(slide_texts) + f"\nNotes:\n{notes_text}\n"
                    elif slide_texts: # Only slide text, no notes
                         text += "\n".join(slide_texts) + "\n"
                 elif slide_texts: # Only slide text, no notes slide or empty notes
                      text += "\n".join(slide_texts) + "\n"

            elif slide_texts: # No notes slide, but has slide text
                 text += "\n".join(slide_texts) + "\n"

            text += "\n" # Separator between slides content (or just adds newline if slide was empty)
        return text.strip() if text else None
    except Exception as e:
        print(f"Error reading PPTX {os.path.basename(file_path)}: {e}")
        return None


def extract_text_from_txt(file_path):
    try:
        # Try common encodings
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        for enc in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    return f.read()
            except UnicodeDecodeError:
                continue
            except Exception as inner_e: # Catch other file reading errors
                 print(f"Error reading TXT {os.path.basename(file_path)} with encoding {enc}: {inner_e}")
                 return None # Stop if a non-decoding error occurs

        # If all encodings fail
        print(f"Error reading TXT {os.path.basename(file_path)}: Could not decode using {encodings_to_try}")
        # Optionally, try reading as bytes and decoding with replacement
        # with open(file_path, 'rb') as f:
        #     raw_content = f.read()
        # return raw_content.decode('utf-8', errors='replace')
        return None

    except Exception as e:
        print(f"Error accessing TXT file {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_csv(file_path):
    try:
        # Read CSV, try to handle potential parsing issues
        df = pd.read_csv(file_path, error_bad_lines=False, warn_bad_lines=True)
        # Convert entire DataFrame to string for context
        return df.to_string(index=False, na_rep='NA').strip()
    except Exception as e:
        print(f"Error reading CSV {os.path.basename(file_path)}: {e}")
        return None

# --- Main Processing Logic ---

def process_documents(file_paths, questions_to_ask):
    """
    Processes a list of local document files, generates embeddings,
    and answers questions based on the most relevant document.
    """
    all_documents = []
    file_names = []

    print("\n--- Processing Local Documents ---")
    print(f"Files to process: {file_paths}")

    for file_path in file_paths:
        if not os.path.exists(file_path):
            print(f"Warning: File not found - {file_path}. Skipping.")
            continue

        file_name = os.path.basename(file_path)
        print(f"Reading file: {file_name}")
        _, ext = os.path.splitext(file_path.lower()) # Use lower case extension
        doc_text = None

        if ext == ".pdf":
            doc_text = extract_text_from_pdf(file_path)
        elif ext == ".docx":
            doc_text = extract_text_from_docx(file_path)
        elif ext == ".txt":
            doc_text = extract_text_from_txt(file_path)
        elif ext in [".xlsx", ".xls"]:
            doc_text = extract_text_from_excel(file_path)
        elif ext == ".pptx":
            doc_text = extract_text_from_pptx(file_path)
        elif ext == ".csv":
            doc_text = extract_text_from_csv(file_path)
        else:
            print(f"Warning: Unsupported file type '{ext}' for {file_name}. Skipping.")
            continue

        if doc_text and doc_text.strip(): # Ensure extracted text is not empty/whitespace
            all_documents.append(doc_text)
            file_names.append(file_name)
            print(f"Successfully processed and added text from {file_name}.")
        elif doc_text is not None:
             print(f"Extracted text from {file_name} was empty. Skipping.")
        else:
             print(f"Failed to extract text from {file_name}.")


    if not all_documents:
        print("No documents were successfully processed or contained text. Exiting document processing.")
        return [] # Return empty list

    # Generate embeddings
    embeddings, index, processed_file_names = generate_embeddings(all_documents, file_names)

    if index is None:
        print("Failed to generate embeddings or index. Cannot answer questions.")
        return [] # Return empty list

    # Initialize QA Agent
    # === MODIFIED PART ===
    print(f"Initializing CodeAgent with HfApiModel using model: {HF_MODEL_PATH}")
    try:
        # Explicitly pass the model path to HfApiModel
        agent = CodeAgent(tools=[], model=HfApiModel(model=HF_MODEL_PATH))
        print("CodeAgent initialized successfully.")
    except Exception as e:
        print(f"!!! Critical Error: Failed to initialize CodeAgent with model {HF_MODEL_PATH}: {e}")
        print("!!! Please ensure the model path is correct and the model files are accessible.")
        print("!!! Check if 'transformers' library and necessary model dependencies are installed.")
        return [] # Cannot proceed without the agent
    # ====================


    # Ask questions
    print("\n--- Answering Questions for Documents ---")
    results = []
    for question in questions_to_ask:
        relevant_doc, distance, rel_file_name = find_relevant_document(question, index, all_documents, processed_file_names)

        if relevant_doc is None:
            print(f"Could not find relevant document for question: '{question}'")
            answer = "Could not find a relevant document to answer the question."
            rel_file_name = "N/A"
            distance = "N/A"
        else:
            answer = generate_answer_with_agent(agent, question, relevant_doc)

        print("-" * 20)
        print(f"Question: {question}")
        print(f"Most Relevant File: {rel_file_name}")
        dist_str = f"{distance:.4f}" if isinstance(distance, (float, np.float32)) else str(distance)
        print(f"Relevance Score (Distance): {dist_str}")
        print(f"Answer:\n{answer}")
        print("-" * 20 + "\n")
        results.append({
            'question': question,
            'file': rel_file_name,
            'distance': distance, # Keep original distance value
            'answer': answer
        })
    return results


def process_website(url, questions_to_ask):
    """
    Processes a website URL using Firecrawl, generates embeddings for the
    scraped content, and answers questions.
    """
    print("\n--- Processing Website ---")
    print(f"URL to process: {url}")

    if not FIRECRAWL_API_KEY:
        print("Error: Firecrawl API key not set (FIRECRAWL_API environment variable). Skipping website processing.")
        return []

    if not url:
        print("Error: No URL provided for website processing.")
        return []

    documents = []
    file_names = [url] # Use the URL as the identifier

    try:
        print("Initializing Firecrawl...")
        app = FirecrawlApp(api_key=FIRECRAWL_API_KEY)
        print(f"Scraping URL: {url}...")
        # Request markdown, focus on main content, simplify structure
        scrape_params = {
            'pageOptions': {
                'onlyMainContent': True,
                'includeHtml': False, # Don't need HTML if we get markdown
            },
             'extractorOptions': {
                 'mode': 'markdown',
                 'extractionSchema': 'type: text' # Try to get just text
            }
        }
        # Use crawl_url for potentially better structure handling, fallback to scrape_url
        try:
            # Firecrawl >= 0.2.3 crawl_url returns a list of documents
            crawl_result = app.crawl_url(url, params=scrape_params, wait_until_done=True, crawler_options={'max_depth': 0}) # Depth 0 for single page
            if crawl_result and isinstance(crawl_result, list) and 'markdown' in crawl_result[0]:
                scraped_text = crawl_result[0]['markdown']
                print("Extracted Markdown content using crawl_url.")
            else:
                 raise ValueError("crawl_url did not return expected markdown format.")
        except Exception as crawl_e:
            print(f"crawl_url failed ({crawl_e}), trying scrape_url...")
            scrape_result = app.scrape_url(url, params=scrape_params)
            scraped_text = None
            if isinstance(scrape_result, dict):
                if 'markdown' in scrape_result and scrape_result['markdown']:
                    scraped_text = scrape_result['markdown']
                    print("Extracted Markdown content using scrape_url.")
                elif 'content' in scrape_result and scrape_result['content']:
                    scraped_text = scrape_result['content']
                    print("Extracted 'content' field using scrape_url.")
                else:
                    print("Warning: Could not find 'markdown' or 'content' in scrape_url result.")
                    print("Full scrape result:", scrape_result)
            else:
                 print(f"Warning: Unexpected scrape_url result format: {type(scrape_result)}")
                 print("Full scrape result:", scrape_result)


        if scraped_text and scraped_text.strip():
            documents.append(scraped_text.strip())
            print(f"Successfully scraped content from {url}.")
        else:
            print(f"No valid content extracted for {url}. Please check the URL and Firecrawl result.")
            return [] # Can't proceed without content

    except Exception as e:
        print(f"Error during website scraping for {url}: {e}")
        return []

    if not documents:
        print("No content scraped from the website. Cannot proceed.")
        return []

    # Generate embeddings for the single scraped document
    embeddings, index, processed_file_names = generate_embeddings(documents, file_names)

    if index is None:
        print("Failed to generate embeddings or index for website content.")
        return []

    # Initialize QA Agent
    # === MODIFIED PART ===
    print(f"Initializing CodeAgent with HfApiModel using model: {HF_MODEL_PATH}")
    try:
        # Explicitly pass the model path to HfApiModel
        agent = CodeAgent(tools=[], model=HfApiModel(model=HF_MODEL_PATH))
        print("CodeAgent initialized successfully.")
    except Exception as e:
        print(f"!!! Critical Error: Failed to initialize CodeAgent with model {HF_MODEL_PATH}: {e}")
        # No fallback here, just report and exit this function
        return []
    # ====================

    # Ask questions
    print("\n--- Answering Questions for Website ---")
    results = []
    for question in questions_to_ask:
        # Since there's only one document, Faiss search is trivial but keeps the flow consistent
        relevant_doc, distance, rel_file_name = find_relevant_document(question, index, documents, processed_file_names)

        if relevant_doc is None:
             print(f"Error retrieving the scraped document for question: '{question}'")
             answer = "Could not retrieve the scraped document."
             rel_file_name = url # Best identifier we have
             distance = "N/A"
        else:
            answer = generate_answer_with_agent(agent, question, relevant_doc)

        print("-" * 20)
        print(f"Question: {question}")
        print(f"Source URL: {rel_file_name}")
        dist_str = f"{distance:.4f}" if isinstance(distance, (float, np.float32)) else str(distance)
        # Distance might be less meaningful for a single doc search, but keep for consistency
        # print(f"Relevance Score (Distance): {dist_str}")
        print(f"Answer:\n{answer}")
        print("-" * 20 + "\n")

        results.append({
            'question': question,
            'url': rel_file_name,
            'answer': answer
            # 'distance': distance # Distance is less informative here
        })
    return results

# --- Main Execution ---
if __name__ == "__main__":
    # --- Define Inputs Here ---

    # 1. Document Processing
    # List of local file paths to process - ADD YOUR FILES HERE
    document_files = [
         "/content/NASDAQ_AMZN_2022.pdf",
        # Example: "reports/annual_report.docx",
    ]
    # Questions to ask about the documents
    document_questions = [
        "What is the main topic discussed?",
        "Summarize the key findings or conclusions.",
        "Are there any specific recommendations mentioned?",
        # Add more specific questions relevant to your documents
    ]

    # 2. Website Processing
    # URL of the website to process - REPLACE WITH YOUR URL or set to None
    website_url = None # Example: "https://firecrawl.dev"
    # Questions to ask about the website
    website_questions = [
        "What is the main purpose of this website?",
        "Summarize the content of the page.",
        "What services or products are offered?",
        "Who is the target audience?",
    ]

    # --- Run Processing ---

    all_doc_results = []
    if document_files:
        document_results = process_documents(document_files, document_questions)
        if document_results:
            print("\n=== Document Processing Summary ===")
            for res in document_results:
                 print(f"Q: {res['question']} (File: {res['file']}) -> A: {res['answer'][:150]}...") # Print summary
            all_doc_results.extend(document_results)
            # Optionally save detailed results to a file (e.g., JSON)
            try:
                with open("document_qa_results.json", "w", encoding='utf-8') as f:
                    json.dump(document_results, f, indent=4, ensure_ascii=False)
                print("Document results saved to document_qa_results.json")
            except Exception as e:
                print(f"Error saving document results to JSON: {e}")
        else:
            print("\nDocument processing did not yield results.")

    all_web_results = []
    if website_url:
        website_results = process_website(website_url, website_questions)
        if website_results:
            print("\n=== Website Processing Summary ===")
            for res in website_results:
                 print(f"Q: {res['question']} (URL: {res['url']}) -> A: {res['answer'][:150]}...") # Print summary
            all_web_results.extend(website_results)
            # Optionally save detailed results
            try:
                with open("website_qa_results.json", "w", encoding='utf-8') as f:
                    json.dump(website_results, f, indent=4, ensure_ascii=False)
                print("Website results saved to website_qa_results.json")
            except Exception as e:
                 print(f"Error saving website results to JSON: {e}")
        else:
             print("\nWebsite processing did not yield results.")

    print("\nScript finished.")

    # You can access the combined results if needed:
    # combined_results = all_doc_results + all_web_results

import numpy as np
# Faiss not needed for single doc processing
from transformers import AutoTokenizer, AutoModel # Keep for potential use by QA/embedding model
import torch
import os
import fitz # PyMuPDF
import pandas as pd
from io import StringIO, BytesIO
import docx
from pptx import Presentation
import json
# Removed Firecrawl, Opik, Tkinter, threading imports
from smolagents import CodeAgent, HfApiModel
import sys # To exit gracefully

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Embedding Model Path (can be local path or HF name)
MODEL_PATH = os.getenv("MODEL_PATH", "sentence-transformers/all-MiniLM-L6-v2")
print(f"Using Embedding Model: {MODEL_PATH}")

# QA Generation Model Path - Set to the specific local path
HF_MODEL_PATH = "/content/Dolphin3.0-Llama3.2-1B" # MAKE SURE THIS PATH IS CORRECT
print(f"Using QA Model Path: {HF_MODEL_PATH}")

# --- Model Initialization ---
# Embedding Model (Load minimally, primarily for QA agent if needed)
embedding_tokenizer = None
embedding_model = None
try:
    print("Loading embedding model (if needed)...")
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    embedding_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    embedding_model = AutoModel.from_pretrained(MODEL_PATH)
    embedding_model.to(device)
    print("Embedding model loaded.")
except Exception as e:
    print(f"Warning: Could not load embedding model ({MODEL_PATH}): {e}")
    embedding_model = None

# QA Agent
qa_agent = None
try:
    print(f"Initializing QA Agent with model: {HF_MODEL_PATH}...")
    hf_api_model = HfApiModel(model=HF_MODEL_PATH) # Initialize the model wrapper
    qa_agent = CodeAgent(tools=[], model=hf_api_model) # Pass the wrapper to the agent
    print("QA Agent initialized.")
except Exception as e:
    print(f"!!! Critical Error: Failed to initialize QA Agent with model {HF_MODEL_PATH}: {e}")
    print("!!! Please ensure the model path is correct, accessible, and dependencies are installed.")
    qa_agent = None # Indicate failure

# --- Core QA Function (Modified for Single Document) ---

def generate_answer_for_doc(agent, question, document_text):
    """Generates an answer using the agent based on the provided text."""
    if not agent:
         print("Error: QA Agent not initialized successfully.")
         return None # Return None to indicate failure
    if not document_text:
        print("Error: No document text provided or extracted.")
        return None
    if not question:
        print("Error: No question provided.")
        return None

    # Consider chunking large documents if the model has context limits
    # For simplicity, we send the whole text here.
    # MAX_CONTEXT_LENGTH = 4096 # Example limit, adjust based on Dolphin model
    # if len(document_text) > MAX_CONTEXT_LENGTH * 1.2: # Heuristic for token count
    #     print(f"Warning: Document text is long ({len(document_text)} chars), potentially exceeding context limit.")
        # Implement chunking or summarization here if needed

    prompt = f"Based *only* on the following document, answer the question precisely and concisely.\n\nDocument:\n{document_text}\n\nQuestion: {question}\n\nAnswer:"
    print(f"\nGenerating answer for question: '{question}'...")
    print("Please wait, this may take some time depending on the model and document size...")
    try:
        response = agent.run(prompt)
        answer = str(response).strip() # Ensure string and remove leading/trailing whitespace
        print("Answer generation complete.")
        return answer
    except Exception as e:
        print(f"Error during answer generation: {e}")
        return None

# --- File Processing Helper Functions (Ensure they return None on error) ---

def extract_text_from_pdf(file_path):
    try:
        with fitz.open(file_path) as pdf_file:
            text = "".join(page.get_text() for page in pdf_file)
            return text if text.strip() else None
    except Exception as e:
        print(f"Error reading PDF {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_docx(file_path):
    try:
        doc = docx.Document(file_path)
        text = "\n".join(para.text for para in doc.paragraphs)
        return text if text.strip() else None
    except Exception as e:
        print(f"Error reading DOCX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_excel(file_path):
    try:
        excel_file = pd.ExcelFile(file_path)
        text = ""
        for sheet_name in excel_file.sheet_names:
            # Skip hidden sheets if possible (though pandas doesn't directly support this easily)
            # Basic check: Skip sheets starting with common hidden prefixes if desired
            # if sheet_name.startswith(('Sheet', 'Feuil', '_')): # Example filter
            df = excel_file.parse(sheet_name)
            if not df.empty: # Process only non-empty sheets
                text += f"--- Sheet: {sheet_name} ---\n"
                # Use a more LLM-friendly string representation if possible
                # to_markdown might be good if `tabulate` is installed
                try:
                     text += df.to_markdown(index=False, numalign="left", stralign="left") + "\n\n"
                except ImportError:
                     text += df.to_string(index=False, na_rep='NA').strip() + "\n\n"
            # else:
            #     print(f"Skipping empty sheet: {sheet_name}")
        return text.strip() if text.strip() else None
    except Exception as e:
        print(f"Error reading Excel {os.path.basename(file_path)}: {e}")
        return None


def extract_text_from_pptx(file_path):
    try:
        prs = Presentation(file_path)
        text = ""
        for i, slide in enumerate(prs.slides):
            slide_texts = []
            # Check for shapes containing text
            for shape in slide.shapes:
                 if hasattr(shape, "text"):
                     shape_text = shape.text.strip() if shape.text else ""
                     if shape_text:
                         slide_texts.append(shape_text)

            # Check for notes text
            notes_text = ""
            try:
                if slide.has_notes_slide:
                    notes_slide = slide.notes_slide
                    if notes_slide.notes_text_frame and notes_slide.notes_text_frame.text:
                        notes_text = notes_slide.notes_text_frame.text.strip()
            except Exception as note_e:
                 # Don't stop extraction for notes error, just warn
                 print(f"Warning: Could not read notes for slide {i+1} in {os.path.basename(file_path)}: {note_e}")


            # Combine slide text and notes if either exists
            if slide_texts or notes_text:
                text += f"--- Slide {i+1} ---\n"
                if slide_texts:
                    text += "Slide Text:\n" + "\n".join(slide_texts) + "\n"
                if notes_text:
                    text += f"Notes:\n{notes_text}\n"
                text += "\n" # Add separation after content of each slide

        return text.strip() if text.strip() else None # Return None if no text found
    except Exception as e:
        print(f"Error reading PPTX {os.path.basename(file_path)}: {e}")
        return None


def extract_text_from_txt(file_path):
    try:
        # Try common encodings
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        content = None
        for enc in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    content = f.read()
                # If successful read, break the loop
                break
            except UnicodeDecodeError:
                continue # Try next encoding
            except Exception as inner_e:
                 print(f"Error reading TXT {os.path.basename(file_path)} with encoding {enc}: {inner_e}")
                 return None # Return None on other read errors

        if content is None:
             print(f"Error reading TXT {os.path.basename(file_path)}: Could not decode using {encodings_to_try}")
             return None
        else:
             return content if content.strip() else None

    except FileNotFoundError:
         print(f"Error: TXT file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error accessing TXT file {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_csv(file_path):
    try:
        # Try common encodings for CSV
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        df = None
        for enc in encodings_to_try:
            try:
                # Try reading with basic error handling
                df = pd.read_csv(file_path, encoding=enc, low_memory=False, on_bad_lines='warn')
                break # Success
            except UnicodeDecodeError:
                continue
            except Exception as read_err:
                 print(f"Error reading CSV {os.path.basename(file_path)} with encoding {enc}: {read_err}")
                 return None # Stop if error other than encoding

        if df is None:
             print(f"Error reading CSV {os.path.basename(file_path)}: Could not decode or parse.")
             return None

        # Convert DataFrame to a string representation
        # to_markdown is often more readable for LLMs if tabulate is installed
        try:
            csv_string = df.to_markdown(index=False, numalign="left", stralign="left")
        except ImportError:
            print("`tabulate` not installed, using basic string format for CSV.")
            csv_string = df.to_string(index=False, na_rep='NA').strip()

        return csv_string if csv_string else None
    except FileNotFoundError:
         print(f"Error: CSV file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error processing CSV {os.path.basename(file_path)}: {e}")
        return None

def get_document_text(filepath):
    """Extracts text based on file extension."""
    if not filepath or not os.path.exists(filepath):
        print(f"Error: File not found at the provided path: {filepath}")
        return None

    filename = os.path.basename(filepath)
    _, ext = os.path.splitext(filename.lower())
    print(f"\nAttempting to extract text from: {filename} (Type: {ext})")
    doc_text = None

    if ext == ".pdf":
        doc_text = extract_text_from_pdf(filepath)
    elif ext == ".docx":
        doc_text = extract_text_from_docx(filepath)
    elif ext == ".txt":
        doc_text = extract_text_from_txt(filepath)
    elif ext in [".xlsx", ".xls"]:
        doc_text = extract_text_from_excel(filepath)
    elif ext == ".pptx":
        doc_text = extract_text_from_pptx(filepath)
    elif ext == ".csv":
        doc_text = extract_text_from_csv(filepath)
    else:
        print(f"Error: Unsupported file type '{ext}'. Supported types: pdf, docx, txt, xlsx, pptx, csv")
        return None

    if doc_text:
        print(f"Successfully extracted text from {filename}.")
        # Optional: Print summary or length
        # print(f"Extracted text length: {len(doc_text)} characters.")
        # print(f"Text preview:\n{doc_text[:500]}...\n")
        return doc_text
    else:
        print(f"Failed to extract text from {filename}, or the file is empty/unreadable.")
        return None


# --- Main Execution Logic ---
if __name__ == "__main__":
    # Check if QA Agent loaded - Exit if not.
    if qa_agent is None:
        print("\nQA Agent failed to initialize. Exiting script.")
        sys.exit(1) # Exit with a non-zero code to indicate error

    print("\n--- Local Document Q&A Console ---")
    print("Enter the full path to the document file you want to query.")
    print("Supported types: pdf, docx, txt, xlsx, pptx, csv")

    while True:
        # 1. Get File Path
        file_path = input("\nEnter document path (or type 'quit' to exit): ").strip()
        if file_path.lower() == 'quit':
            break
        if not file_path:
            continue

        # 2. Extract Text
        document_text = get_document_text(file_path)
        if document_text is None:
            # Error message already printed by get_document_text
            continue # Ask for path again

        # 3. Get Question
        question = input("Enter your question about this document: ").strip()
        if not question:
            print("No question entered. Please try again.")
            continue

        # 4. Generate Answer
        answer = generate_answer_for_doc(qa_agent, question, document_text)

        # 5. Display Results
        print("\n--- Query Result ---")
        print(f"Document: {os.path.basename(file_path)}")
        print(f"Question: {question}")
        print("-" * 20)
        if answer is not None:
            print(f"Answer:\n{answer}")
        else:
            print("Answer: Failed to generate an answer.")
        print("-" * 20)

    print("\nExiting Document Q&A.")
    print("Script finished.")

"""aÿ¥ÿ∫ÿßŸÑ ŸÖÿπ ŸÉÿ™ÿßÿ® ÿµÿ∫Ÿäÿ± ÿØŸàŸÉÿ≥"""

import numpy as np
# Faiss not needed for single doc processing
from transformers import AutoTokenizer, AutoModel # Keep for potential use by QA/embedding model
import torch
import os
import fitz # PyMuPDF
import pandas as pd
from io import StringIO, BytesIO
import docx
from pptx import Presentation
import json
# Removed Firecrawl, Opik, Tkinter, threading imports
from smolagents import CodeAgent, HfApiModel
import sys # To exit gracefully

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Embedding Model Path (can be local path or HF name)
MODEL_PATH = os.getenv("MODEL_PATH", "sentence-transformers/all-MiniLM-L6-v2")
print(f"Using Embedding Model: {MODEL_PATH}")

# QA Generation Model Path - Set to the specific local path
# === IMPORTANT: Ensure this environment variable is set if HfApiModel uses it ===
HF_MODEL_PATH_ENV_VAR = "SMOL_HF_MODEL" # Example name, check smolagents docs if needed
HF_MODEL_PATH_VALUE = "/content/Dolphin3.0-Llama3.2-1B" # MAKE SURE THIS PATH IS CORRECT
os.environ[HF_MODEL_PATH_ENV_VAR] = HF_MODEL_PATH_VALUE
print(f"Using QA Model Path (set via env var {HF_MODEL_PATH_ENV_VAR}): {HF_MODEL_PATH_VALUE}")
# ============================================================================

# --- Model Initialization ---
# Embedding Model (Load minimally, primarily for QA agent if needed)
embedding_tokenizer = None
embedding_model = None
try:
    print("Loading embedding model (if needed)...")
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    embedding_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    embedding_model = AutoModel.from_pretrained(MODEL_PATH)
    embedding_model.to(device)
    print("Embedding model loaded.")
except Exception as e:
    print(f"Warning: Could not load embedding model ({MODEL_PATH}): {e}")
    embedding_model = None

# QA Agent
qa_agent = None
try:
    print(f"Initializing QA Agent using environment settings for model path...")
    # === CORRECTED INITIALIZATION ===
    # Initialize HfApiModel without the 'model' keyword argument.
    # It might pick up the model path from an environment variable (like SMOL_HF_MODEL or HF_MODEL_PATH)
    # or have a default behavior.
    hf_api_model = HfApiModel()
    # ===============================
    qa_agent = CodeAgent(tools=[], model=hf_api_model) # Pass the wrapper to the agent
    print("QA Agent initialized.")
    # Optional: Check which model HfApiModel actually loaded if possible
    # if hasattr(hf_api_model, 'model_name'): # Example attribute check
    #    print(f"  -> HfApiModel loaded: {hf_api_model.model_name}")
    # elif hasattr(hf_api_model, 'pipeline'): # Check underlying pipeline
    #    print(f"  -> HfApiModel pipeline model: {hf_api_model.pipeline.model.name_or_path}")

except Exception as e:
    print(f"!!! Critical Error: Failed to initialize QA Agent: {e}")
    print("!!! Check if the required model is accessible and if environment variables (if used by smolagents) are set correctly.")
    qa_agent = None # Indicate failure

# --- Core QA Function (Modified for Single Document) ---
# (generate_answer_for_doc remains the same)
def generate_answer_for_doc(agent, question, document_text):
    """Generates an answer using the agent based on the provided text."""
    if not agent:
         print("Error: QA Agent not initialized successfully.")
         return None # Return None to indicate failure
    if not document_text:
        print("Error: No document text provided or extracted.")
        return None
    if not question:
        print("Error: No question provided.")
        return None

    prompt = f"Based *only* on the following document, answer the question precisely and concisely.\n\nDocument:\n{document_text}\n\nQuestion: {question}\n\nAnswer:"
    print(f"\nGenerating answer for question: '{question}'...")
    print("Please wait, this may take some time depending on the model and document size...")
    try:
        response = agent.run(prompt)
        answer = str(response).strip() # Ensure string and remove leading/trailing whitespace
        print("Answer generation complete.")
        return answer
    except Exception as e:
        print(f"Error during answer generation: {e}")
        # You might want to see the full traceback for debugging generation errors
        import traceback
        traceback.print_exc()
        return None


# --- File Processing Helper Functions ---
# (extract_text_from_pdf, docx, excel, pptx, txt, csv remain the same)
def extract_text_from_pdf(file_path):
    try:
        with fitz.open(file_path) as pdf_file:
            text = "".join(page.get_text() for page in pdf_file)
            return text if text.strip() else None
    except Exception as e:
        print(f"Error reading PDF {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_docx(file_path):
    try:
        doc = docx.Document(file_path)
        text = "\n".join(para.text for para in doc.paragraphs)
        return text if text.strip() else None
    except Exception as e:
        print(f"Error reading DOCX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_excel(file_path):
    try:
        excel_file = pd.ExcelFile(file_path)
        text = ""
        for sheet_name in excel_file.sheet_names:
            df = excel_file.parse(sheet_name)
            if not df.empty:
                text += f"--- Sheet: {sheet_name} ---\n"
                try:
                     text += df.to_markdown(index=False, numalign="left", stralign="left") + "\n\n"
                except ImportError:
                     text += df.to_string(index=False, na_rep='NA').strip() + "\n\n"
        return text.strip() if text.strip() else None
    except Exception as e:
        print(f"Error reading Excel {os.path.basename(file_path)}: {e}")
        return None


def extract_text_from_pptx(file_path):
    try:
        prs = Presentation(file_path)
        text = ""
        for i, slide in enumerate(prs.slides):
            slide_texts = []
            for shape in slide.shapes:
                 if hasattr(shape, "text"):
                     shape_text = shape.text.strip() if shape.text else ""
                     if shape_text:
                         slide_texts.append(shape_text)
            notes_text = ""
            try:
                if slide.has_notes_slide:
                    notes_slide = slide.notes_slide
                    if notes_slide.notes_text_frame and notes_slide.notes_text_frame.text:
                        notes_text = notes_slide.notes_text_frame.text.strip()
            except Exception as note_e:
                 print(f"Warning: Could not read notes for slide {i+1} in {os.path.basename(file_path)}: {note_e}")
            if slide_texts or notes_text:
                text += f"--- Slide {i+1} ---\n"
                if slide_texts:
                    text += "Slide Text:\n" + "\n".join(slide_texts) + "\n"
                if notes_text:
                    text += f"Notes:\n{notes_text}\n"
                text += "\n"
        return text.strip() if text.strip() else None
    except Exception as e:
        print(f"Error reading PPTX {os.path.basename(file_path)}: {e}")
        return None


def extract_text_from_txt(file_path):
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        content = None
        for enc in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
            except Exception as inner_e:
                 print(f"Error reading TXT {os.path.basename(file_path)} with encoding {enc}: {inner_e}")
                 return None
        if content is None:
             print(f"Error reading TXT {os.path.basename(file_path)}: Could not decode using {encodings_to_try}")
             return None
        else:
             return content if content.strip() else None
    except FileNotFoundError:
         print(f"Error: TXT file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error accessing TXT file {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_csv(file_path):
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        df = None
        for enc in encodings_to_try:
            try:
                df = pd.read_csv(file_path, encoding=enc, low_memory=False, on_bad_lines='warn')
                break # Success
            except UnicodeDecodeError:
                continue
            except Exception as read_err:
                 print(f"Error reading CSV {os.path.basename(file_path)} with encoding {enc}: {read_err}")
                 return None # Stop if error other than encoding
        if df is None:
             print(f"Error reading CSV {os.path.basename(file_path)}: Could not decode or parse.")
             return None
        try:
            csv_string = df.to_markdown(index=False, numalign="left", stralign="left")
        except ImportError:
            #print("`tabulate` not installed, using basic string format for CSV.")
            csv_string = df.to_string(index=False, na_rep='NA').strip()
        return csv_string if csv_string else None
    except FileNotFoundError:
         print(f"Error: CSV file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error processing CSV {os.path.basename(file_path)}: {e}")
        return None

# (get_document_text remains the same)
def get_document_text(filepath):
    """Extracts text based on file extension."""
    if not filepath or not os.path.exists(filepath):
        print(f"Error: File not found at the provided path: {filepath}")
        return None

    filename = os.path.basename(filepath)
    _, ext = os.path.splitext(filename.lower())
    print(f"\nAttempting to extract text from: {filename} (Type: {ext})")
    doc_text = None

    if ext == ".pdf":
        doc_text = extract_text_from_pdf(filepath)
    elif ext == ".docx":
        doc_text = extract_text_from_docx(filepath)
    elif ext == ".txt":
        doc_text = extract_text_from_txt(filepath)
    elif ext in [".xlsx", ".xls"]:
        doc_text = extract_text_from_excel(filepath)
    elif ext == ".pptx":
        doc_text = extract_text_from_pptx(filepath)
    elif ext == ".csv":
        doc_text = extract_text_from_csv(filepath)
    else:
        print(f"Error: Unsupported file type '{ext}'. Supported types: pdf, docx, txt, xlsx, pptx, csv")
        return None

    if doc_text:
        print(f"Successfully extracted text from {filename}.")
        return doc_text
    else:
        print(f"Failed to extract text from {filename}, or the file is empty/unreadable.")
        return None


# --- Main Execution Logic ---
if __name__ == "__main__":
    # Check if QA Agent loaded - Exit if not.
    if qa_agent is None:
        print("\nQA Agent failed to initialize. Exiting script.")
        sys.exit(1) # Exit with a non-zero code to indicate error

    print("\n--- Local Document Q&A Console ---")
    print("Enter the full path to the document file you want to query.")
    print("Supported types: pdf, docx, txt, xlsx, pptx, csv")

    while True:
        # 1. Get File Path
        file_path = input("\nEnter document path (or type 'quit' to exit): ").strip()
        if file_path.lower() == 'quit':
            break
        if not file_path:
            continue

        # 2. Extract Text
        document_text = get_document_text(file_path)
        if document_text is None:
            continue # Ask for path again

        # 3. Get Question
        question = input("Enter your question about this document: ").strip()
        if not question:
            print("No question entered. Please try again.")
            continue

        # 4. Generate Answer
        answer = generate_answer_for_doc(qa_agent, question, document_text)

        # 5. Display Results
        print("\n--- Query Result ---")
        print(f"Document: {os.path.basename(file_path)}")
        print(f"Question: {question}")
        print("-" * 20)
        if answer is not None:
            print(f"Answer:\n{answer}")
        else:
            print("Answer: Failed to generate an answer. Check console for errors during generation.")
        print("-" * 20)

    print("\nExiting Document Q&A.")
    print("Script finished.")



"""ÿ¥ÿ∫ÿßŸÑ ÿ±ÿßÿ¶ÿπ"""

import numpy as np
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline # Added imports
import torch
import os
import fitz
import pandas as pd
from io import StringIO, BytesIO
import docx
from pptx import Presentation
import json
from smolagents import CodeAgent, HfApiModel
import sys
import math # For ceiling division

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

MODEL_PATH = os.getenv("MODEL_PATH", "sentence-transformers/all-MiniLM-L6-v2")
print(f"Using Embedding Model: {MODEL_PATH}")

# === IMPORTANT: Ensure this environment variable is set if HfApiModel uses it ===
HF_MODEL_PATH_ENV_VAR = "SMOL_HF_MODEL"
HF_MODEL_PATH_VALUE = "/content/Dolphin3.0-Llama3.2-1B" # MAKE SURE THIS PATH IS CORRECT
os.environ[HF_MODEL_PATH_ENV_VAR] = HF_MODEL_PATH_VALUE
print(f"Using QA Model Path (set via env var {HF_MODEL_PATH_ENV_VAR}): {HF_MODEL_PATH_VALUE}")

# === Add Chunking Configuration ===
# Estimate tokens conservatively (e.g., 3 chars/token) or use tokenizer if loaded
# Let's target ~4000 tokens per chunk to be safe for many models.
TARGET_CHUNK_SIZE_CHARS = 4000 * 3 # Approx characters
CHUNK_OVERLAP_CHARS = 200 * 3 # Approx overlap
# ==================================


# --- Model Initialization ---
embedding_tokenizer = None
embedding_model = None
try:
    print("Loading embedding model (if needed)...")
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
    embedding_tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    embedding_model = AutoModel.from_pretrained(MODEL_PATH)
    embedding_model.to(device)
    print("Embedding model loaded.")
except Exception as e:
    print(f"Warning: Could not load embedding model ({MODEL_PATH}): {e}")
    embedding_model = None


qa_agent = None
try:
    print(f"Initializing QA Agent using environment settings for model path...")
    hf_api_model = HfApiModel()
    qa_agent = CodeAgent(tools=[], model=hf_api_model)
    print("QA Agent initialized.")
except Exception as e:
    print(f"!!! Critical Error: Failed to initialize QA Agent: {e}")
    qa_agent = None

# --- Text Splitting Function ---
def split_text(text, chunk_size=TARGET_CHUNK_SIZE_CHARS, chunk_overlap=CHUNK_OVERLAP_CHARS):
    if not text:
        return []
    chunks = []
    start_index = 0
    text_len = len(text)
    while start_index < text_len:
        end_index = min(start_index + chunk_size, text_len)
        chunks.append(text[start_index:end_index])
        start_index += chunk_size - chunk_overlap # Move start index, considering overlap
        if start_index >= end_index: # Avoid infinite loop if overlap >= size
             start_index = end_index
    print(f"Split text into {len(chunks)} chunks.")
    return chunks


# --- Core QA Function (Modified for Chunking - Simple Approach) ---
def generate_answer_from_chunks(agent, question, document_chunks):
    if not agent:
        print("Error: QA Agent not initialized successfully.")
        return None
    if not document_chunks:
        print("Error: No document chunks provided.")
        return None
    if not question:
        print("Error: No question provided.")
        return None

    print(f"\nGenerating answer for question: '{question}' by checking {len(document_chunks)} chunks...")
    print("Please wait...")

    all_answers = []
    # --- Simple approach: Ask question for each chunk ---
    for i, chunk in enumerate(document_chunks):
        print(f"  Processing chunk {i+1}/{len(document_chunks)}...")
        prompt = f"Based *only* on the following text chunk, answer the question precisely and concisely. If the answer is not in this chunk, say 'Answer not found in this chunk'.\n\nText Chunk:\n{chunk}\n\nQuestion: {question}\n\nAnswer:"

        try:
            response = agent.run(prompt)
            answer = str(response).strip()
            # Basic check if the model found something relevant
            if "not found in this chunk" not in answer.lower() and len(answer) > 10: # Heuristic
                print(f"    -> Found potential answer in chunk {i+1}: {answer[:100]}...")
                all_answers.append(answer)
                # OPTIONAL: Stop after the first good answer for speed
                # break
            else:
                 print(f"    -> Answer not found or irrelevant in chunk {i+1}.")

        except Exception as e:
            print(f"  Error processing chunk {i+1}: {e}")
            # Optionally print traceback for debugging chunk errors
            # import traceback
            # traceback.print_exc()
            continue # Skip to next chunk

    # --- Combine or select the best answer ---
    if not all_answers:
        return "Could not find a relevant answer in any document chunk."
    elif len(all_answers) == 1:
        return all_answers[0]
    else:
        # More sophisticated combination needed (e.g., ask LLM to synthesize)
        # For now, just return the first good answer found.
        print(f"Found multiple potential answers ({len(all_answers)}). Returning the first one.")
        return all_answers[0]

# --- File Processing Helpers (Unchanged) ---
def extract_text_from_pdf(file_path):
    # ... (same as before)
    try:
        with fitz.open(file_path) as pdf_file:
            text = "".join(page.get_text() for page in pdf_file)
            return text if text.strip() else None
    except Exception as e:
        print(f"Error reading PDF {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_docx(file_path):
    # ... (same as before)
    try:
        doc = docx.Document(file_path)
        text = "\n".join(para.text for para in doc.paragraphs)
        return text if text.strip() else None
    except Exception as e:
        print(f"Error reading DOCX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_excel(file_path):
    # ... (same as before)
    try:
        excel_file = pd.ExcelFile(file_path)
        text = ""
        for sheet_name in excel_file.sheet_names:
            df = excel_file.parse(sheet_name)
            if not df.empty:
                text += f"--- Sheet: {sheet_name} ---\n"
                try:
                     text += df.to_markdown(index=False, numalign="left", stralign="left") + "\n\n"
                except ImportError:
                     text += df.to_string(index=False, na_rep='NA').strip() + "\n\n"
        return text.strip() if text.strip() else None
    except Exception as e:
        print(f"Error reading Excel {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_pptx(file_path):
    # ... (same as before)
    try:
        prs = Presentation(file_path)
        text = ""
        for i, slide in enumerate(prs.slides):
            slide_texts = []
            for shape in slide.shapes:
                 if hasattr(shape, "text"):
                     shape_text = shape.text.strip() if shape.text else ""
                     if shape_text:
                         slide_texts.append(shape_text)
            notes_text = ""
            try:
                if slide.has_notes_slide:
                    notes_slide = slide.notes_slide
                    if notes_slide.notes_text_frame and notes_slide.notes_text_frame.text:
                        notes_text = notes_slide.notes_text_frame.text.strip()
            except Exception as note_e:
                 print(f"Warning: Could not read notes for slide {i+1} in {os.path.basename(file_path)}: {note_e}")
            if slide_texts or notes_text:
                text += f"--- Slide {i+1} ---\n"
                if slide_texts:
                    text += "Slide Text:\n" + "\n".join(slide_texts) + "\n"
                if notes_text:
                    text += f"Notes:\n{notes_text}\n"
                text += "\n"
        return text.strip() if text.strip() else None
    except Exception as e:
        print(f"Error reading PPTX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_txt(file_path):
    # ... (same as before)
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        content = None
        for enc in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
            except Exception as inner_e:
                 print(f"Error reading TXT {os.path.basename(file_path)} with encoding {enc}: {inner_e}")
                 return None
        if content is None:
             print(f"Error reading TXT {os.path.basename(file_path)}: Could not decode using {encodings_to_try}")
             return None
        else:
             return content if content.strip() else None
    except FileNotFoundError:
         print(f"Error: TXT file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error accessing TXT file {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_csv(file_path):
    # ... (same as before)
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        df = None
        for enc in encodings_to_try:
            try:
                df = pd.read_csv(file_path, encoding=enc, low_memory=False, on_bad_lines='warn')
                break # Success
            except UnicodeDecodeError:
                continue
            except Exception as read_err:
                 print(f"Error reading CSV {os.path.basename(file_path)} with encoding {enc}: {read_err}")
                 return None # Stop if error other than encoding
        if df is None:
             print(f"Error reading CSV {os.path.basename(file_path)}: Could not decode or parse.")
             return None
        try:
            csv_string = df.to_markdown(index=False, numalign="left", stralign="left")
        except ImportError:
            csv_string = df.to_string(index=False, na_rep='NA').strip()
        return csv_string if csv_string else None
    except FileNotFoundError:
         print(f"Error: CSV file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error processing CSV {os.path.basename(file_path)}: {e}")
        return None

# (get_document_text remains the same)
def get_document_text(filepath):
    # ... (same as before)
    if not filepath or not os.path.exists(filepath):
        print(f"Error: File not found at the provided path: {filepath}")
        return None
    filename = os.path.basename(filepath)
    _, ext = os.path.splitext(filename.lower())
    print(f"\nAttempting to extract text from: {filename} (Type: {ext})")
    doc_text = None
    if ext == ".pdf": doc_text = extract_text_from_pdf(filepath)
    elif ext == ".docx": doc_text = extract_text_from_docx(filepath)
    elif ext == ".txt": doc_text = extract_text_from_txt(filepath)
    elif ext in [".xlsx", ".xls"]: doc_text = extract_text_from_excel(filepath)
    elif ext == ".pptx": doc_text = extract_text_from_pptx(filepath)
    elif ext == ".csv": doc_text = extract_text_from_csv(filepath)
    else: print(f"Error: Unsupported file type '{ext}'."); return None
    if doc_text: print(f"Successfully extracted text from {filename}."); return doc_text
    else: print(f"Failed to extract text from {filename}, or the file is empty/unreadable."); return None


# --- Main Execution Logic (Updated) ---
if __name__ == "__main__":
    if qa_agent is None:
        print("\nQA Agent failed to initialize. Exiting script.")
        sys.exit(1)

    print("\n--- Local Document Q&A Console ---")
    # ... (rest of the intro prints)

    while True:
        file_path = input("\nEnter document path (or type 'quit' to exit): ").strip()
        if file_path.lower() == 'quit': break
        if not file_path: continue

        document_text = get_document_text(file_path)
        if document_text is None: continue

        # === Add Chunking Step ===
        document_chunks = split_text(document_text)
        if not document_chunks:
             print("Error: Failed to split document into chunks.")
             continue
        # =========================

        question = input("Enter your question about this document: ").strip()
        if not question: print("No question entered."); continue

        # === Use Chunking Function ===
        answer = generate_answer_from_chunks(qa_agent, question, document_chunks)
        # ===========================

        print("\n--- Query Result ---")
        print(f"Document: {os.path.basename(file_path)}")
        print(f"Question: {question}")
        print("-" * 20)
        if answer is not None:
            print(f"Answer:\n{answer}")
        else:
            print("Answer: Failed to generate an answer. Check console for errors during generation.")
        print("-" * 20)

    print("\nExiting Document Q&A.")
    print("Script finished.")





"""ÿ¥ÿ∫ÿßŸÑ ÿµÿ≠ ÿ®ÿ≥ ÿ®ÿ∑ÿ¶"""

import numpy as np
# Core transformers imports for pipeline
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, pipeline
import torch
import os
import fitz
import pandas as pd
from io import StringIO, BytesIO
import docx
from pptx import Presentation
import json
# Removed smolagents imports as we bypass it for QA now
# from smolagents import CodeAgent, HfApiModel
import sys
import math
import traceback # For detailed error printing

# --- Configuration ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Embedding Model Configuration (Optional, kept for potential future use)
EMBEDDING_MODEL_PATH = os.getenv("EMBEDDING_MODEL_PATH", "sentence-transformers/all-MiniLM-L6-v2")
print(f"Using Embedding Model (if needed): {EMBEDDING_MODEL_PATH}")

# QA Model Configuration (Local Path)
QA_MODEL_PATH = "/content/Dolphin3.0-Llama3.2-1B" # MAKE SURE THIS PATH IS CORRECT
print(f"Using QA Model Path (local): {QA_MODEL_PATH}")

# === Chunking Configuration ===
TARGET_CHUNK_SIZE_CHARS = 4000 * 3 # Approx characters for ~4k tokens
CHUNK_OVERLAP_CHARS = 200 * 3 # Approx overlap
MAX_ANSWER_TOKENS = 512 # Max tokens for the generated answer in pipeline
# =============================

# --- Environment Variable Check ---
# Warn user if HF_TOKEN is set, as transformers might use it implicitly
if os.getenv("HF_TOKEN"):
    print("\n--- WARNING ---")
    print("The HF_TOKEN environment variable is set.")
    print("While this script aims for local loading, the transformers library")
    print("might still use this token for checks or if local files are missing.")
    print("Unset HF_TOKEN environment variable to prevent potential Hub communication.")
    print("---------------\n")

# --- Model Initialization ---

# 1. Embedding Model (Optional Load)
embedding_tokenizer = None
embedding_model = None
try:
    print("Loading embedding model (if needed)...")
    os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" # Keep if needed
    embedding_tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_PATH, local_files_only=True) # Try local first
    embedding_model = AutoModel.from_pretrained(EMBEDDING_MODEL_PATH, local_files_only=True) # Try local first
    embedding_model.to(device)
    print("Embedding model loaded (attempted local first).")
except Exception as e:
    print(f"Warning: Could not load embedding model locally ({EMBEDDING_MODEL_PATH}): {e}")
    print("Set local_files_only=False if download is acceptable.")
    embedding_model = None # Ensure it's None if loading failed

# 2. QA Model and Pipeline (Explicit Local Loading)
qa_pipeline = None
try:
    print(f"Initializing local QA Pipeline from path: {QA_MODEL_PATH}...")
    # Load tokenizer and model explicitly from the local path
    # Set local_files_only=True to prevent any potential download attempts
    qa_tokenizer = AutoTokenizer.from_pretrained(QA_MODEL_PATH, local_files_only=True)
    qa_model = AutoModelForCausalLM.from_pretrained(
        QA_MODEL_PATH,
        local_files_only=True,
        device_map="auto", # Automatically use GPU if available, else CPU
        torch_dtype="auto" # Automatically select appropriate dtype
        # Consider adding bitsandbytes quantization if memory is an issue:
        # load_in_8bit=True # or load_in_4bit=True (requires bitsandbytes)
    )
    print("Local QA model loaded.")

    # Create the text generation pipeline
    # Ensure the task matches the model's capability (usually text-generation for causal LMs)
    qa_pipeline = pipeline(
        "text-generation",
        model=qa_model,
        tokenizer=qa_tokenizer,
        # Optional: Add default generation parameters here
        # max_new_tokens=MAX_ANSWER_TOKENS, # Set max answer length
        # temperature=0.7,
        # top_p=0.9,
        # do_sample=True,
        # Add model-specific kwargs if needed
    )
    print("Local QA Pipeline initialized successfully.")

except Exception as e:
    print(f"!!! Critical Error: Failed to initialize local QA Pipeline from {QA_MODEL_PATH}: {e}")
    print("!!! Please ensure the path is correct, the model files are complete,")
    print("!!! and necessary libraries (transformers, torch, accelerate, etc.) are installed.")
    traceback.print_exc() # Print full traceback for debugging
    qa_pipeline = None # Indicate failure


# --- Text Splitting Function ---
# (split_text remains the same)
def split_text(text, chunk_size=TARGET_CHUNK_SIZE_CHARS, chunk_overlap=CHUNK_OVERLAP_CHARS):
    if not text:
        return []
    chunks = []
    start_index = 0
    text_len = len(text)
    while start_index < text_len:
        end_index = min(start_index + chunk_size, text_len)
        chunks.append(text[start_index:end_index])
        start_index += chunk_size - chunk_overlap # Move start index, considering overlap
        if start_index >= end_index: # Avoid infinite loop if overlap >= size
             start_index = end_index
    print(f"Split text into {len(chunks)} chunks.")
    return chunks


# --- Core QA Function (Using direct pipeline call) ---
def generate_answer_from_chunks_pipeline(pipe, question, document_chunks):
    if not pipe:
        print("Error: QA Pipeline not initialized successfully.")
        return None
    if not document_chunks:
        print("Error: No document chunks provided.")
        return None
    if not question:
        print("Error: No question provided.")
        return None

    print(f"\nGenerating answer for question: '{question}' by checking {len(document_chunks)} chunks...")
    print("Please wait...")

    all_answers = []
    # --- Simple approach: Ask question for each chunk ---
    for i, chunk in enumerate(document_chunks):
        print(f"  Processing chunk {i+1}/{len(document_chunks)}...")
        # Construct the prompt for the pipeline
        # Note: Different models might prefer different prompt formats. Adjust if needed.
        prompt = f"Based *only* on the following text chunk, answer the question precisely and concisely. If the answer is not in this chunk, say 'Answer not found in this chunk'.\n\nText Chunk:\n{chunk}\n\nQuestion: {question}\n\nAnswer:"

        try:
            # Call the pipeline directly
            # Pipeline returns a list of dictionaries, e.g., [{'generated_text': '... answer ...'}]
            # We need to extract the actual text *after* the prompt.
            # Set max_length carefully = prompt length + max new tokens
            # Or use max_new_tokens directly if supported well by the pipeline task.
            # Let's estimate prompt tokens roughly or get exact count if needed.
            # For simplicity, rely on max_new_tokens.
            response = pipe(
                prompt,
                max_new_tokens=MAX_ANSWER_TOKENS, # Max length *for the answer itself*
                return_full_text=False, # IMPORTANT: Only return the generated part
                # Add other generation params if needed:
                # temperature=0.7,
                # top_p=0.9,
                # do_sample=True,
            )

            # Extract answer from pipeline output
            if response and isinstance(response, list) and 'generated_text' in response[0]:
                answer = response[0]['generated_text'].strip()
            else:
                 print(f"    -> Unexpected pipeline output format for chunk {i+1}: {response}")
                 answer = "" # Treat as not found

            # Basic check if the model found something relevant
            if answer and "not found in this chunk" not in answer.lower() and len(answer) > 5: # Adjusted heuristic
                print(f"    -> Found potential answer in chunk {i+1}: {answer[:100]}...")
                all_answers.append(answer)
                # OPTIONAL: Stop after the first good answer for speed
                # break
            else:
                 # Print the negative answer only if it exists, otherwise it was unexpected output
                 if answer:
                      print(f"    -> Answer not found or irrelevant in chunk {i+1} (Model said: {answer[:50]}...).")
                 else:
                      print(f"    -> Answer not found or irrelevant in chunk {i+1} (Unexpected output).")


        except Exception as e:
            print(f"  Error processing chunk {i+1} with pipeline: {e}")
            traceback.print_exc() # Print full traceback for chunk errors
            continue # Skip to next chunk

    # --- Combine or select the best answer ---
    if not all_answers:
        return "Could not find a relevant answer in any document chunk."
    elif len(all_answers) == 1:
        return all_answers[0]
    else:
        print(f"Found multiple potential answers ({len(all_answers)}). Returning the first one.")
        # TODO: Implement better combination strategy if needed (e.g., synthesize with another LLM call)
        return all_answers[0]

# --- File Processing Helpers (Unchanged) ---
def extract_text_from_pdf(file_path):
    # ... (same as before)
    try:
        with fitz.open(file_path) as pdf_file:
            text = "".join(page.get_text() for page in pdf_file)
            return text if text.strip() else None
    except Exception as e:
        print(f"Error reading PDF {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_docx(file_path):
    # ... (same as before)
    try:
        doc = docx.Document(file_path)
        text = "\n".join(para.text for para in doc.paragraphs)
        return text if text.strip() else None
    except Exception as e:
        print(f"Error reading DOCX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_excel(file_path):
    # ... (same as before)
    try:
        excel_file = pd.ExcelFile(file_path)
        text = ""
        for sheet_name in excel_file.sheet_names:
            df = excel_file.parse(sheet_name)
            if not df.empty:
                text += f"--- Sheet: {sheet_name} ---\n"
                try:
                     text += df.to_markdown(index=False, numalign="left", stralign="left") + "\n\n"
                except ImportError:
                     text += df.to_string(index=False, na_rep='NA').strip() + "\n\n"
        return text.strip() if text.strip() else None
    except Exception as e:
        print(f"Error reading Excel {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_pptx(file_path):
    # ... (same as before)
    try:
        prs = Presentation(file_path)
        text = ""
        for i, slide in enumerate(prs.slides):
            slide_texts = []
            for shape in slide.shapes:
                 if hasattr(shape, "text"):
                     shape_text = shape.text.strip() if shape.text else ""
                     if shape_text:
                         slide_texts.append(shape_text)
            notes_text = ""
            try:
                if slide.has_notes_slide:
                    notes_slide = slide.notes_slide
                    if notes_slide.notes_text_frame and notes_slide.notes_text_frame.text:
                        notes_text = notes_slide.notes_text_frame.text.strip()
            except Exception as note_e:
                 print(f"Warning: Could not read notes for slide {i+1} in {os.path.basename(file_path)}: {note_e}")
            if slide_texts or notes_text:
                text += f"--- Slide {i+1} ---\n"
                if slide_texts:
                    text += "Slide Text:\n" + "\n".join(slide_texts) + "\n"
                if notes_text:
                    text += f"Notes:\n{notes_text}\n"
                text += "\n"
        return text.strip() if text.strip() else None
    except Exception as e:
        print(f"Error reading PPTX {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_txt(file_path):
    # ... (same as before)
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        content = None
        for enc in encodings_to_try:
            try:
                with open(file_path, 'r', encoding=enc) as f:
                    content = f.read()
                break
            except UnicodeDecodeError:
                continue
            except Exception as inner_e:
                 print(f"Error reading TXT {os.path.basename(file_path)} with encoding {enc}: {inner_e}")
                 return None
        if content is None:
             print(f"Error reading TXT {os.path.basename(file_path)}: Could not decode using {encodings_to_try}")
             return None
        else:
             return content if content.strip() else None
    except FileNotFoundError:
         print(f"Error: TXT file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error accessing TXT file {os.path.basename(file_path)}: {e}")
        return None

def extract_text_from_csv(file_path):
    # ... (same as before)
    try:
        encodings_to_try = ['utf-8', 'latin-1', 'cp1252']
        df = None
        for enc in encodings_to_try:
            try:
                df = pd.read_csv(file_path, encoding=enc, low_memory=False, on_bad_lines='warn')
                break # Success
            except UnicodeDecodeError:
                continue
            except Exception as read_err:
                 print(f"Error reading CSV {os.path.basename(file_path)} with encoding {enc}: {read_err}")
                 return None # Stop if error other than encoding
        if df is None:
             print(f"Error reading CSV {os.path.basename(file_path)}: Could not decode or parse.")
             return None
        try:
            csv_string = df.to_markdown(index=False, numalign="left", stralign="left")
        except ImportError:
            csv_string = df.to_string(index=False, na_rep='NA').strip()
        return csv_string if csv_string else None
    except FileNotFoundError:
         print(f"Error: CSV file not found at {file_path}")
         return None
    except Exception as e:
        print(f"Error processing CSV {os.path.basename(file_path)}: {e}")
        return None

# (get_document_text remains the same)
def get_document_text(filepath):
    # ... (same as before)
    if not filepath or not os.path.exists(filepath):
        print(f"Error: File not found at the provided path: {filepath}")
        return None
    filename = os.path.basename(filepath)
    _, ext = os.path.splitext(filename.lower())
    print(f"\nAttempting to extract text from: {filename} (Type: {ext})")
    doc_text = None
    if ext == ".pdf": doc_text = extract_text_from_pdf(filepath)
    elif ext == ".docx": doc_text = extract_text_from_docx(filepath)
    elif ext == ".txt": doc_text = extract_text_from_txt(filepath)
    elif ext in [".xlsx", ".xls"]: doc_text = extract_text_from_excel(filepath)
    elif ext == ".pptx": doc_text = extract_text_from_pptx(filepath)
    elif ext == ".csv": doc_text = extract_text_from_csv(filepath)
    else: print(f"Error: Unsupported file type '{ext}'."); return None
    if doc_text: print(f"Successfully extracted text from {filename}."); return doc_text
    else: print(f"Failed to extract text from {filename}, or the file is empty/unreadable."); return None


# --- Main Execution Logic (Using direct pipeline) ---
if __name__ == "__main__":
    # === Critical Check: Ensure QA pipeline loaded ===
    if qa_pipeline is None:
        print("\nLocal QA Pipeline failed to initialize. Cannot proceed. Exiting.")
        sys.exit(1) # Exit if pipeline setup failed
    # ================================================

    print("\n--- Local Document Q&A Console (Using Direct Pipeline) ---")
    print("This script uses models loaded directly from local paths.")
    print("It does not rely on external APIs or smolagents for QA.")
    print(f"QA Model: {QA_MODEL_PATH}")
    print("Enter the full path to the document file you want to query.")
    print("Supported types: pdf, docx, txt, xlsx, pptx, csv")


    while True:
        file_path = input("\nEnter document path (or type 'quit' to exit): ").strip()
        if file_path.lower() == 'quit': break
        if not file_path: continue

        document_text = get_document_text(file_path)
        if document_text is None: continue

        document_chunks = split_text(document_text)
        if not document_chunks:
             print("Error: Failed to split document into chunks.")
             continue

        question = input("Enter your question about this document: ").strip()
        if not question: print("No question entered."); continue

        # === Use Direct Pipeline Function ===
        answer = generate_answer_from_chunks_pipeline(qa_pipeline, question, document_chunks)
        # ====================================

        print("\n--- Query Result ---")
        print(f"Document: {os.path.basename(file_path)}")
        print(f"Question: {question}")
        print("-" * 20)
        if answer is not None:
            print(f"Answer:\n{answer}")
        else:
            print("Answer: Failed to generate an answer. Check console for errors during generation.")
        print("-" * 20)

    print("\nExiting Document Q&A.")
    print("Script finished.")

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = "tiiuae/falcon-rw-1b"

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")

from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

# Change this to the path to your locally saved model
model_path = "/content/falcon-rw-1b"

tokenizer = AutoTokenizer.from_pretrained(model_path)
pipeline = transformers.pipeline(
    "text-generation",
    model=model_path, # Using the model path
    tokenizer=tokenizer,
    torch_dtype="auto",
    device_map="auto",
)
sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    max_length=50,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")



from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

# Change this to the path to your locally saved model
model_path = "/content/falcon-rw-1b"

tokenizer = AutoTokenizer.from_pretrained(model_path)
pipeline = transformers.pipeline(
    "text-generation",
    model=model_path, # Using the model path
    tokenizer=tokenizer,
    torch_dtype="auto",
    device_map="auto",
)
sequences = pipeline(
   "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:",
    # Increase max_length or use max_new_tokens:
    max_length=100,  # Increased to allow for generated text
    # max_new_tokens=50,  # Alternative: generate 50 new tokens
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f"Result: {seq['generated_text']}")



from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

# Change this to the path to your locally saved model
model_path = "/content/falcon-rw-1b"

tokenizer = AutoTokenizer.from_pretrained(model_path)
pipeline = transformers.pipeline(
    "text-generation",
    model=model_path,
    tokenizer=tokenizer,
    torch_dtype="auto",
    device_map="auto",
)

# The question in Arabic:
question = "Who is Napoleon Bonaparte?"

sequences = pipeline(
    question,
    max_length=200,  # Adjust as needed for the expected response length
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)

for seq in sequences:
    print(f"Result: {seq['generated_text']}")

"""please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`"""

